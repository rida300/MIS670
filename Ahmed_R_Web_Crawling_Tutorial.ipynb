{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawling procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**The following program extracts all outgoing links from a webpage and crawls each of these links. It can be thought of as a recursive process since the program will store a list of all hyperlinks on a given page, crawl that page and then crawl all the links it collected which would again involve collecting the outgoing links from these links before any crawling is done. \n",
    "It first creates a directory for the project, then it creates 2 files named ‘crawled’ and ‘queue’.  The queue file stores a list of all the href links on the homepage whose url is provided by the user. To make it user-friendly, the only user input requires for this process is this base url. Next, it crawls the homepage, then stores all the links from the first page in the queue and crawls this page. Once a webpage has been crawled, its url is moved into the 'crawled' file while the url(s) that have yet to be crawled exist in the 'queue' file. To ensure that we only crawl one given directory, our program ensures that the domain name of the pages it crawls matches the domain of the base url. This prevents it from crawling the entire internet.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following cell imports the necessary packages for this part of the tutorial. Below is an explanation of why they are needed:\n",
    "**Import urlparse: This package allows us to split a URL string into its components, or sometimes do the reverse by combining URL components into a URL string. \n",
    "The function usually parses the URL into six components, returning a tuple of 6 items. This corresponds to the general structure of a URL:   scheme://netloc/path;parameters?query#fragment Each tuple item is a string, possibly empty. The components are not broken up in smaller parts (for example, the network location is a single string), and % escapes are not expanded. The (; , ? , # ) are not part of the result, except for a leading slash in the path component, which is retained if present. For example: if the following url ('http://www.cwi.nl:80/%7Eguido/Python.html') is parsed using ‘urlparse’, we get the following results: \n",
    "scheme='http', netloc='www.cwi.nl:80', path='/%7Eguido/Python.html', parameters='', query='', fragment=''\n",
    "As seen here, the parameters, query and fragment fields are left empty here since the given url is simple enough.**\n",
    "\n",
    "**Import HTMLParser: An HTMLParser receives HTML data and calls handler methods when start tags, end tags, text, comments, and other markup elements are encountered. If we want this parser to behave differently and produce different results, it is possible to subclass it and override its methods to achieve this.**\n",
    "\n",
    "**Import urlopen: Open the URL url, which can be either a string or a Request object.\n",
    "data may be a string specifying additional data to send to the server, or None if no such data is needed. Currently HTTP requests are the only ones that use data; the HTTP request will be a POST instead of a GET when the data parameter is provided. data should be a buffer in the standard application/x-www-form-urlencoded format. The urllib.parse.urlencode() function takes a mapping or sequence of 2-tuples and returns a string in this format\n",
    "urllib.request is a module used for fetching URLs. One of its most important functions is urlopen which is capable of fetching URLs using a variety of different protocols. It also offers a slightly more complex interface for handling common situations - like basic authentication, cookies, proxies and so on. This module can be used to fetch URLs for many URL schemes using their associated network protocols e.g. FTP or HTTP. (identified by the string before the \":\" in URL - for example \"ftp\" is the URL scheme of ftp://python.org/)**\n",
    "\n",
    "**Import Queue: This provides a constructor for a FIFO (first in first out) queue. This is defined by the queue class as  *class queue.Queue(maxsize=0)* where maxsize is an integer that sets the upperbound limit on the number of items that can be placed in the queue. If maxsize is less than or equal to zero, the queue has an infinite size.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the following program requires python 3 environment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import os\n",
    "from html.parser import HTMLParser\n",
    "from urllib import parse\n",
    "from urllib.request import urlopen\n",
    "import threading\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a method that creates a project directory if and only if the same project does not already exist. In Python, functions are defined using a 'def' statement. The general form look like this** \n",
    "\n",
    "**def function-name(Parameter list):**\n",
    "\n",
    "**&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;statements, i.e. the function body**\n",
    "\n",
    "**The parameter list in this method is directory. Line#3 specifically checks if this folder already exists before creating it. There is a print statement right after this check to let the user know if the program found the project. The last line is one that creates the directory.  **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a method to create the project directory if it is a new project\n",
    "def create_project_dir(directory):\n",
    "    if not os.path.exists(directory):#only create if it doesn't already exist\n",
    "        print('Creating project '+ directory)\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_project_dir('theFirstProject') #only prints the name of the project when this cell is run for the first time because after that, it is already created and the if statement returns a false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method that writes to files: The parameter list consists of the file path and the data that has to be written to this file. This method also takes care of closing the file after it has written to it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the new file:\n",
    "def write_file(path, data):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(data)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method to append data on files: The same parameter list as the method above. This method starts writing at the nd of the file and ensures that each link is added on a new line by using the newline character. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to add data onto an existing file\n",
    "def append_to_file(path, data):\n",
    "    with open(path, 'a') as file:\n",
    "        file.write(data+'\\n') #each link on a new line  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method to delete the contents of a file: The paarmeter list consists of 1 paarmeter which is the path of the file. This method is responsible for emptying the file by deleting all of its contents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to delete the conetnts of a file\n",
    "#creates a new file of the same name: i.e: opens that file and erases its contents\n",
    "def delete_file_contents(path):\n",
    "    open(path, 'w').close() \n",
    "    \n",
    "#write mode selected  #pass # do nothing\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method to create the crawled and queue files: The parameters required include the name of the project which is a string type and the second paarmeter is the base url which is usually the url of the homepage. This url will be provided by the user. Line#3 and 4 create the files with the filename being the name of the project followed by an indication of which of the two types of file is being created. The following if blocks are included o make sure that these files do not already exist.(in case we are running this program for the second time on the same directory.) In Line#6, the queue file is being written to so that it contains the base url; this ensures that when the program starts, we have one url in the waiting list. The crawled file is created so it is empty as of now since no webpages have been crawled yet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating queue and crawled files(if they don't already exist)\n",
    "def create_data_files(project_name, base_url):\n",
    "    queue=os.path.join(project_name+ '/queue.txt')\n",
    "    crawled= os.path.join(project_name+'/crawled.txt')\n",
    "    if not os.path.isfile(queue):\n",
    "        write_file(queue, base_url) # so that when the program starts, it has one url in the waiting list\n",
    "    if not os.path.isfile(crawled):\n",
    "        write_file(crawled, '') #empty file so that the program knows this url has not been crawled yet\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_data_files('theFirstProject', 'http://us.asos.com/women/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**However, this process is slower than if variables were used instead of a bunch of methods. The advantage of this process\n",
    "is that if the system accidentally shuts down or crashes, we still have the data we crawled saved in the 2 files. If variables were being used, the entire process would have to be repeated. Tehrefore, we will use both variables and methods in this program.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method to convert the file into a set: The method takes one parameter which is the filename. A set can contain each item only once. This method makes sure that we don't crawl one page multiple times which only slows down the process. It converts the links in each file into a set. Line#4 reads the text file (rt) line by line and adds each link to the set. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read a file and convert each line to set items\n",
    "def file_to_set(file_name):\n",
    "    results=set()\n",
    "    with open(file_name, 'rt') as f:\n",
    "        for line in f:\n",
    "            results.add(line.replace('\\n', ''))#deletes the newline part of the url\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method to convert a set to a file: The first parameter is the name of the set and the second is the name of the file. First, the file is cleared by calling the delete_file_contents method. Then we sort the set so all the links are now in lphabetical order. The method then iterates over these links and writes them to the file one link per line. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through a set, each item in the set will be a new line in the file\n",
    "def set_to_file(links, file_name):\n",
    "    delete_file_contents(file_name)\n",
    "    with open(file_name, \"w\") as f:\n",
    "        for link in sorted(links):#alphabetican order\n",
    "            f.write(link + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the Links on this base webpage (whose url is provided above):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This class is amde to inherit from the HTMLParser so it has the functionality of an HTMLParser.**\n",
    "\n",
    "**The first method is the initializer method which calls the initializer method of the super class. This method initializes the homepage's url. We create a set to store all the links extracted from the homepage, this is named self.links.  **\n",
    "\n",
    "**handle_starttag is a method in HTMLParser. We are overriding this method to check if the tag passed in as aparameter is an 'a' tag which means we have a link. If it is, we create a for loop to iterate through its attributes (attrs). The attributes of an 'a' tag refer to the components that it consist of for example: href, class, etc are attributes and what they are set to is their value. Each iteration is over the tuple of attribute and value in the attrs field. Since we only want to store the url, we specifically look for the href attribute, then we concatenate the url to the base url to create the full url and add it to the set.**\n",
    "\n",
    "**page_links is a method to return teh set of links that was created in the initializer method. **\n",
    "\n",
    "**The error method needs to be implemented everytime a class inherits from the HTMLParser class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkFinder(HTMLParser):\n",
    "    def __init__(self, base_url, page_url):\n",
    "        super().__init__()\n",
    "        self.base_url = base_url\n",
    "        self.page_url = page_url\n",
    "        self.links = set()\n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'a':\n",
    "            for(attribute, value) in attrs: #stored in a tuple\n",
    "                if attribute == 'href':#collecting the url of this link\n",
    "                    #converting a relative url to one with a full domain name\n",
    "                    url = parse.urljoin(self.base_url, value)#if it is a full url, it is saved as it is. Otherwise, the relative url is combined with the base url and then saved \n",
    "                    self.links.add(url)\n",
    "                    \n",
    "                    \n",
    "    def page_links(self):\n",
    "        return self.links\n",
    "                    \n",
    "    def error(self, message):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the spider class:\n",
    "**We will have a bunch of links in our waiting list, waiting to be crawled. The spider will hold on to one of these links, connect to that page and grab all of the html of this webpage and feed it to the Linkfinder object which return all of the links found in this html. Once spider has all of the links from this webpage, it processes each link and makes sure that it isn't already in the waiting list and that it hasn't already been crawled after which it adds the link to the waiting list. The Spider class also moves the webpage it has just extracted the links from into the crawled_links file to ensure that a page is not crawled twice.**\n",
    "\n",
    "**The class first declares variables for the name of the project, the url of the homepage, the domain name of this url and the queue and crawled files and their corresponding sets. **\n",
    "\n",
    "**The initializer method sets teh values of these variables and calls the crawl_page method.**\n",
    "\n",
    "**The boot method is a static one which means we don't need to create an instance of this class in order to call this method. This method is mainly responsible for calling the appropriate methods that create a folder and the 'crawled' and 'queue' files for the project. It also calls the method to convert tehse files to sets again just so we don't crawl one page more than once. **\n",
    "\n",
    "**The crawl_page method takes 2 parameters: first is the thread_name to let the user know which page is being crawled. In the initializer method, when crawl_page is called, we passed in 'First Spider' which meant the program is carwling the first page. The second paaameter is the url of the pag eto be crawled. The method checks if this url is present in the crawled set and if it isn't, it proceeds to process and crawl it. Line#34 prints the number of links in the wueue file and those in the crawled file to give the user an idea of how long it's going to take to complete the process. The next line extracts all the links on this page and adds them to the queue. We now need to remove this webpage that we are currently working on from the queue and add it to the crawled set.**\n",
    "\n",
    "**The gather_links method connects to the site and receives the html code which is initially in binary form. It \n",
    "converts this into actual html format, passes it onto the linkFinder which parses through it and extracs all the links from this page. It creates a variable of the string datatype. There is a try and catch block on the part where it links to the webpage and receives the html of the page. The if statement checks that we retrieve html code and not any other format such as pdf or an executable. The next line is responsible for formatting the binary code into readable html form. The method then creates an instance of the LinkFinder class and calls the feed function by passing the html code we retrieved. The rest of the methods don’t need to be called manually. The except block contains the code that will be executed if the code in the try block throws an exception. This includes printing the exception and returning an empty set since this method has a return type of set and can’t be exited without returning one. Finally, outside of the except block, we return the updated page links set.**\n",
    "\n",
    "**The add_links_to_queue method takes one parameter which is a set of links and is responsible for adding the extracted links to the queue. It contains a for loop to iterate through each url in this set and checks if it is present in either one of the sets (crawled or queue); if so, the continue keyword instructs the method to move on to the next url in the set. There is also a check to ensure that the domain name of the url matches that of the base url, if it doesn’t, we don’t need to crawl this page.**  \n",
    "\n",
    "**The update_file method calls the methods to convert the queue and crawled sets to text files.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spider:\n",
    "   #making aclass variable that is shared among all the spiders \n",
    "    project_name = ''\n",
    "    base_url = ''\n",
    "    domain_name=''\n",
    "    #also need to create variables for the queue and crawled files\n",
    "    queue_file=''#the actual text file\n",
    "    crawled_file=''#any spider cans et the value of these\n",
    "    queue=set()#stored on he RAM as a buffer \n",
    "    crawled=set()\n",
    "    def __init__(self, project_name, base_url, domain_name):\n",
    "        Spider.project_name= project_name\n",
    "        Spider.base_url = base_url\n",
    "        Spider.domain_name= domain_name\n",
    "        Spider.queue_file= Spider.project_name+'/queue.txt'\n",
    "        Spider.crawled_file= Spider.project_name+'/crawled.txt'\n",
    "        self.boot()\n",
    "        self.crawl_page('First spider', Spider.base_url)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def boot():#the first spider created must create the prohect directory and the 2 data files(queue and crawled)\n",
    "        create_project_dir(Spider.project_name)\n",
    "        create_data_files(Spider.project_name, Spider.base_url)#first spider therefore the url to the homepage is passed in\n",
    "        Spider.queue = file_to_set(Spider.queue_file)\n",
    "        Spider.crawled = file_to_set(Spider.crawled_file)\n",
    "        \n",
    "   \n",
    "\n",
    "    @staticmethod\n",
    "    def crawl_page(thread_name, page_url):#adding the base url to the crawled file\n",
    "        if page_url not in Spider.crawled:\n",
    "            print(thread_name+'crawling'+page_url)\n",
    "            print('Queue: '+ str(len(Spider.queue)) + ' | Crawled: '+str(len(Spider.crawled)))\n",
    "            Spider.add_links_to_queue(Spider.gather_links(page_url))\n",
    "            Spider.queue.remove(page_url)#removing this page that has just been crawled so it no longer exists in the queue(the waiting list)\n",
    "            Spider.crawled.add(page_url)\n",
    "            Spider.update_files()\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def gather_links(page_url):\n",
    "        html_string=''\n",
    "        try:\n",
    "            response= urlopen(page_url)\n",
    "            if response.getheader('Content-Type') ==  'text/html':\n",
    "                html_bytes=response.read()#convert the 1's and 0's received from the browser into actual html format\n",
    "                html_string=html_bytes.decode(\"utf-8\")\n",
    "            finder=LinkFinder(Spider.base_url, page_url)\n",
    "            finder.feed(html_string)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            return set()\n",
    "        return finder.page_links()\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def add_links_to_queue(links):\n",
    "        for url in links:\n",
    "            if (url in Spider.queue) or (url in Spider.crawled):\n",
    "                continue\n",
    "            if Spider.domain_name != get_domain_name(url):\n",
    "                continue\n",
    "            Spider.queue.add(url)\n",
    "            \n",
    " #so that the crawler does not crawl the entire internet, the base url mist be present in all the pages being crawled\n",
    "              \n",
    "                        \n",
    "            \n",
    "            \n",
    "    @staticmethod\n",
    "    def update_files():\n",
    "        set_to_file(Spider.queue, Spider.queue_file)\n",
    "        set_to_file(Spider.crawled, Spider.crawled_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from urllib.parse import urlparse\n",
    "except ImportError:\n",
    "     from urlparse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following method extracts the domain name from the URL taht it takes in as a parameter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get domain name(example.com)\n",
    "def get_domain_name(url):\n",
    "    try:\n",
    "        results= get_sub_domain_name(url).split('.')\n",
    "        return results[-2]+'.'+results[-1]#second to the last and the .com\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This method calls the urlparse function on the URL taht it takes in as a parameter and splits it as excplained above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get subdomain name (name.example.com)\n",
    "def get_sub_domain_name(url):\n",
    "    try:\n",
    "        return urlparse(url).netloc\n",
    "    except:\n",
    "        return''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asos.com\n"
     ]
    }
   ],
   "source": [
    "print(get_domain_name('http://us.asos.com/women/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the next cell, we provide user input including the project name and base URL. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME='asos_crawled'#constants convention allcaps\n",
    "HOMEPAGE='http://us.asos.com/women/'\n",
    "DOMAIN_NAME=get_domain_name(HOMEPAGE)\n",
    "QUEUE_FILE= PROJECT_NAME+'/queue.txt'\n",
    "CRAWLED_FILE = PROJECT_NAME+'/crawled.txt'\n",
    "NUMBER_OF_THREADS= 4 #SPECIFIC TO THE OPERATING SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating project asos_crawled\n",
      "First spidercrawlinghttp://us.asos.com/women/\n",
      "Queue: 1 | Crawled: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Spider at 0xa2b93da5c0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queue= Queue()#where Queue is the package that was imported and queue is the thread. So we are contructing an instance of it. \n",
    "Spider(PROJECT_NAME, HOMEPAGE, DOMAIN_NAME)#the first spider is created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The crawl method converts the queue file created above to a set. It then checks if there are any links present in this set, if so it prints the number of links present.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl():\n",
    "#check if there are items in the queue, if so, crawl them\n",
    "    queued_links=file_to_set(QUEUE_FILE)\n",
    "    if len(queued_links)>0:\n",
    "        print(str(len(queued_links))+ 'links in the queue')\n",
    "        create_jobs()#each queue link is a new job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The create_jobs method adds the URLs from the set to the thread queue. The join function makes sure that each thread waits for its turn to do its job and no 2 threads are working at the same time since we need to follow an order of tasks. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jobs():\n",
    "    for link in file_to_set(QUEUE_FILE):\n",
    "        queue.put(link)\n",
    "    queue.join()\n",
    "    crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The create_workers method has a for loop that is used to create threads by passing in another function as its parameter. This method is created later. Setting daemon to true ensures that this thread is killed when the main application exits. We need to call start on each thread to actually create it. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create worker threads which will be killed once the main is exited\n",
    "def create_workers():\n",
    "    for _ in range(NUMBER_OF_THREADS):\n",
    "        t = threading.Thread(target=work)\n",
    "        t.daemon=True\n",
    "        t.start()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gets the URL from the queue, calls the crawl_page method which takes 2 parameters. The first of which is the name of the thread to let the user know where the program is and what it is doing and the second is the url thatvwe got from the queue. Calling the task_done method lets the operating system know that we are done with this URL so that it can send out the garbage collector to do its job and free up some memory. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the enxt job in the queue\n",
    "def work():\n",
    "    while True:\n",
    "        url= queue.get()\n",
    "        Spider.crawl_page(threading.current_thread().name, url)\n",
    "        queue.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_workers()\n",
    "crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawling using Selenium:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selenium is a software tool that can be used to automate web browsers. Selenium comprises of the Selenium web driver, selenium IDE, and selenium grid. Its most popular use is for functional and regression testing; it automates web applications for testing purposes. It is also the core technology in countless other browser automation tools, APIs and frameworks. Selenium is open source software which makes it possible for users all over the world to freely use this software to test applications. Moreover, Selenium supports java, python, C#, PHP, Pearl, Ruby among other programming languages. The different operating systems that support Selenium include Windows, Mac, Linux, iOS and android. Selenium has been updated over the years to work well with all major browsers including Chrome, Firefox, Safari, Opera and even Internet Explorer.**\n",
    "\n",
    "**Selenium also supports parallel test execution which means that it can run the same test case on different browsers parallelly or different test cases on different instances of the same browser. If used with TestNG, selenium web driver can form html reports.**\n",
    "\n",
    "**In this tutorial, we will be using Selenium web driver. This uses a collection of language specific bindings to drive a browser as per the user’s demands.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a program to submit assignments automatically:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following program allows users to submit assignments to the school's website without logging in every time a new assignment has to be submitted.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, we need to place the completed assignments in folders which are named after the classes that the assignments belong to. We need to import os because this module provides a portable way of using operating system dependent functionality. It provides functionality in order to rea/write a file line by line or one character at a time, manipulate paths, creating temporary files and directories, and for high-level file and directory handling.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We create a list of tuples, each element of this list represents an assignment. Tuple is a datatype that stores 2 components. These may or may not have the same datatype. It can be thought of as a pair; it keeps track of 2 things at once. The first item in this tuple corresponds to the class name while the second one gives the name of the file to submit. CompletedAssignments is the name of the main folder that contains subfolders which represent classes We use a function from the os module and an explicit cast to form a list of classes from the main folder 'CompletedAssignments'. We then create a for loop to iterate through this list of subfolders and create a list of all the files in the folder. After this, we insert this assignment into the list 'file_tup'. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('CIS308', 'Testing_File.rtf')\n"
     ]
    }
   ],
   "source": [
    "# os for file management\n",
    "import os\n",
    " \n",
    "file_tup=[] # Constructs a tuple (class, file to submit)\n",
    "submission_dir = 'CompletedAssignments' \n",
    "\n",
    "dir_list = list(os.listdir(submission_dir)) #casting to list datatype\n",
    "#the following for loop goes through all the subfolders in the CompletedAssignments folder\n",
    "for directory in dir_list:\n",
    "    file_list = list(os.listdir(os.path.join(submission_dir, directory))) #casts the assigments for each class into a list\n",
    "    if len(file_list) != 0:\n",
    "        file_tup = (directory, file_list[0]) #the first item in the tuple is the folder name(directory) and the second item is the index of the assignment being submitted\n",
    "    \n",
    "print(file_tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At this point, the program knows which file has to be submitted and whcich directory it is located in.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "# Using Firefox to access web, this browser is contolled by the program we are writing\n",
    "driver = webdriver.Firefox()\n",
    "driver.get('https://canvas.ksu.edu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**This opens up a Firefox browser window and directs us to the canvas login webpage. The user is prompted to enter their ID and password. In order to train the webdriver, we need to provide precise instructions. These include step by step procedures that it can follow i.e: where to click, what to type, etc. To make this work, we will use Selectors. A selector is a unique identifier for an element on a webpage. The selector for any given text, button or link on a webpage can be found by inspecting the html of this feature of the webpage.  **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following line of code will help us locate the id textbox. The cursor is placed in this box so it is ready to take user input.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the id box\n",
    "id_box = driver.find_element_by_name('username')\n",
    "#lands the cursor in the username textbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that the program can locate the username textbox, we can instruct it on how to enter the username. We use the send_keys function to do this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#types the username provided in the username textbox\n",
    "id_box.send_keys('Enter username') #Enter username here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locates the textbox for enetring the password\n",
    "pass_box = driver.find_element_by_name('password')\n",
    "# Typing in the password\n",
    "pass_box.send_keys('Enter password') # Enter password here\n",
    "# Locates the login button\n",
    "login_button = driver.find_element_by_name('submit')\n",
    "# The following command clicks the login button for us\n",
    "login_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The previous cell has led us to the user dashboard which displays all the current courses and a list of assignments that are due shortly is dispalyed on the right side. Now we look for the course we are wanting to submit an assignment for. To do this, we refer to the first item in the tuple we created above. The first part of the tuple gave the name of the class. Then we need a block of if-elseif... else statements to go through the classes, we stop when we've found a match. When we find the right course, we need to place the cursor on it. To do this, we use 'find_element_by_link_text' which is another selector we can find by inspecting the page and provide the full name of the course.  **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locates teh courses button and clicks it\n",
    "courses_button = driver.find_element_by_id('global_nav_courses_link')\n",
    "courses_button.click()\n",
    "# Get the name of the folder\n",
    "folder = file_tup[0]\n",
    "    \n",
    "# Class to select depends on folder\n",
    "if folder == 'MIS670':\n",
    "    class_select = driver.find_element_by_link_text('Soc Med Anal/Web Min(14825)')\n",
    "elif folder == 'STAT510':\n",
    "    class_select = driver.find_element_by_link_text('Intro Prob & Stat 1(12712)')\n",
    "elif folder == 'CIS308':\n",
    "    class_select = driver.find_element_by_link_text('C/C++ Language Lab(10774)')\n",
    "\n",
    "# Click on the specific class\n",
    "class_select.click()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next step is to find the modules button using (link_text which can be found in the html script)and click it. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modules_select = driver.find_element_by_link_text('Modules')\n",
    "#modules_select.click()\n",
    "\n",
    "assignments_select = driver.find_element_by_link_text('Assignments')\n",
    "assignments_select.click()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we need to locate the assignment we are trying to submit. This may require us to scroll down **\n",
    "**We need to scroll depending on which assignment is to be submitted.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.keys import Keys\n",
    "#driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "#driver.execute_script(\"window.scrollTo(0, 1080)\")\n",
    "#driver.execute_script(\"window.scrollTo(0, 1080)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assignment_select = driver.find_element_by_link_text('Python Bootcamp: SocialMediaAnalyticsWebMining')\n",
    "\n",
    "assignmentSub_select = driver.find_element_by_link_text('Lab 07')\n",
    "assignmentSub_select.send_keys(Keys.END)\n",
    "assignmentSub_select.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitPage_select = driver.find_element_by_link_text('Submit Assignment')\n",
    "submitPage_select.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose File button\n",
    "\n",
    "#choose_file = driver.find_element_by_name('attachments[0][uploaded_data]') #CODE\n",
    "\n",
    "# Complete path of the file\n",
    "\n",
    "#file_name= 'Testing_File.rtf' #CODE\n",
    "#file_location = os.path.join(\"C:\\\\CompletedAssignments\\MIS670\\Testing_File.txt\")\n",
    "\n",
    "#file_location = os.path.join(submission_dir, folder, file_name) #CODE\n",
    "# Send the file location to the button\n",
    "#choose_file.send_keys(file_location) #CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose File button\n",
    "driver.find_element_by_name('attachments[0][uploaded_data]').send_keys('C:\\\\Users\\Rida\\Desktop\\MIS670\\CompletedAssignments\\CIS308\\Testing_File.rtf')\n",
    "# Complete path of the file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Closing the canvas app...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Locate submit button and click\n",
    "submit_assignment = driver.find_element_by_id('submit_file_button')\n",
    "submit_assignment.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Selenium to access facebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following program is used to log in to facebook. The next cell directs Firefox's homepage to facebook's login page. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "# Using Firefox to access web, this browser is contolled by the program we are writing\n",
    "fb = webdriver.Firefox()\n",
    "fb.get('https://www.facebook.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maximizing the window\n",
    "fb.maximize_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next cell sets the implicit wait to 20 seconds so that the browser waits for all the web elements to be loaded before we can start using the html of this webpage.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.implicitly_wait(20) #sets implicit wait to 20 seconds for every web element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**if facebook does not respond in 30 seconds, the page shows an error. If this command is not included,the browser will not wait for facebook to respond and instead start crawling a blank page which will throw unexpected exceptions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.set_page_load_timeout(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The webdriver consists of a function to capture screenshots and save them as files. It returns a true if the screenshot was captured**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb.get_screenshot_as_file(\"fb_initial.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Locating the user ID textbox and passing in 'Selenium Driver' as the username using send_keys. Locating the password textbox and passing in 'python' usings end_keys again and then locating the login button and clicking it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.find_element_by_id(\"email\").send_keys(\"Selenium Driver\")\n",
    "fb.find_element_by_name(\"pass\").send_keys(\"python\")\n",
    "fb.find_element_by_id(\"loginbutton\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#capturing a screenshot\n",
    "fb.get_screenshot_as_file(\"fb_final.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Closing the browser window**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    " #closes the browser window\n",
    "fb.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Selenium to navigate YouTube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "driver = webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.youtube.com/user/noobtoprofessional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opens the youtube video but also shows an error:\n",
    "driver.find_element_by_xpath('//a[contains(text(), \"Why you should learn Python Programming\")]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crwaling a custom made webpage by xpath  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "driver = webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"http://econpy.pythonanywhere.com/ex/001.html\") # the url of the webpage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The xpath for buyers is found in one of the attributes (title) of div. We are storing all the buyers in a list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following command returns a list of all the buyers on this webpage\n",
    "buyers= driver.find_elements_by_xpath('//div[@title=\"buyer-name\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.firefox.webelement.FirefoxWebElement (session=\"fe8f5826-574c-430c-b07a-5a9b6737df32\", element=\"01048c61-0262-481d-95f3-6ba1d5502fdd\")>,\n",
       " <selenium.webdriver.firefox.webelement.FirefoxWebElement (session=\"fe8f5826-574c-430c-b07a-5a9b6737df32\", element=\"83e5d502-cf6e-44c5-9c0a-322cd0c61e49\")>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buyers[:2] # we now need to extract the text from this list of buyers for it to make sense to the users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The xpath for the prics lies in one of the attributes (a class named 'item-price') of span. Also storing these in a list **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = driver.find_elements_by_xpath('//span[@class=\"item-price\"]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.firefox.webelement.FirefoxWebElement (session=\"fe8f5826-574c-430c-b07a-5a9b6737df32\", element=\"b048a112-86fd-4614-9c96-883775cb2a16\")>,\n",
       " <selenium.webdriver.firefox.webelement.FirefoxWebElement (session=\"fe8f5826-574c-430c-b07a-5a9b6737df32\", element=\"501fe731-2c80-45c1-ac4d-1bb23c56ff65\")>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The length of the buyers list and that of the prices list must be equal. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(buyers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following for loop displays each buyer and the price they have paid for the items they bought.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carson Busses : $29.95\n",
      "Earl E. Byrd : $8.37\n",
      "Patty Cakes : $15.26\n",
      "Derri Anne Connecticut : $19.25\n",
      "Moe Dess : $19.25\n",
      "Leda Doggslife : $13.99\n",
      "Dan Druff : $31.57\n",
      "Al Fresco : $8.49\n",
      "Ido Hoe : $14.47\n",
      "Howie Kisses : $15.86\n",
      "Len Lease : $11.11\n",
      "Phil Meup : $15.98\n",
      "Ira Pent : $16.27\n",
      "Ben D. Rules : $7.50\n",
      "Ave Sectomy : $50.85\n",
      "Gary Shattire : $14.26\n",
      "Bobbi Soks : $5.68\n",
      "Sheila Takya : $15.00\n",
      "Rose Tattoo : $114.07\n",
      "Moe Tell : $10.09\n"
     ]
    }
   ],
   "source": [
    "num = len(buyers)\n",
    "for i in range(num):\n",
    "    print(buyers[i].text + \" : \" + prices[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxPage= 5\n",
    "maxPageDigit=3 #3 digit page number is the maximum. i.e: the last page has a 3-digit page-number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can store this in a csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.csv', 'w') as f:\n",
    "    f.write(\"Buyers, Price \\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we can populate the file using a for loop that iterates through the pages given above. The second line in the code prefixes the apprpriate amount of zeroes on the single or double digit page numbers. Then we need edit the url so we can account for the page numbers of the pages we want to crawl. Then we direct the browser to each of these URLs one at a time, create buyers and prices list for the page, open the file for appending and write these lists to the file. After crawling through the last page, we can close the browser.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001\n",
      "http://econpy.pythonanywhere.com/ex/001.html\n",
      "002\n",
      "http://econpy.pythonanywhere.com/ex/002.html\n",
      "003\n",
      "http://econpy.pythonanywhere.com/ex/003.html\n",
      "004\n",
      "http://econpy.pythonanywhere.com/ex/004.html\n",
      "005\n",
      "http://econpy.pythonanywhere.com/ex/005.html\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, maxPage+1):\n",
    "    page_num = (maxPageDigit - len(str(i))) * \"0\" + str(i) #prefixes the number of zeroes in the beginning\n",
    "    print (page_num)\n",
    "    url = \"http://econpy.pythonanywhere.com/ex/\" + page_num + \".html\"\n",
    "    print(url)\n",
    "    driver.get(url)\n",
    "    buyers =  driver.find_elements_by_xpath('//div[@title=\"buyer-name\"]')\n",
    "    prices = driver.find_elements_by_xpath('//span[@class=\"item-price\"]')\n",
    "    num_page_items = len(buyers)\n",
    "    with open('result.csv', 'a') as f:\n",
    "        for i in range (num_page_items):\n",
    "            f.write(buyers[i].text + \",\" + prices[i].text + \"\\n\" )\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling with Beautiful Soup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with a parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. We will be using it to scrape data about Graphic cards from the e-commerce website Newegg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen as uReq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url= \"https://www.newegg.com/Video-Cards-Video-Devices/Category/ID-38?Tpk=graphics%20card\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = uReq(url)\n",
    "page_html =client.read()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The constructor for BeautifulSoup requires the html code and the parser that will accompany it during scraping.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsing html\n",
    "page_soup = soup(page_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following line displays the header on the page whose URL is given above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1 class=\"page-title-text\">Video Cards &amp; Video Devices</h1>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_soup.h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Displaying the first paragraph entry on the page whose URL is given above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p>Newegg.com - A great place to buy computers, computer parts, electronics, software, accessories, and DVDs online. With great prices, fast shipping, and top-rated customer service - once you know, you Newegg.</p>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_soup.p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Displaying an emtry form the body's span attribute**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<span class=\"noCSS\">Skip to:</span>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_soup.body.span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using BeautifulSoup, it is possible to look through a webpage's html and use the findAll function to find the objects with name 'div', attribute type 'class' and value 'item-container'.\n",
    "The findAll method traverses the html code, starting at the given point, and finds all the Tags and NavigableString objects that match the criteria we provide. The signature for the findall method is this:**\n",
    "\n",
    "**findAll(name=None, attrs={}, recursive=True, text=None, limit=None, **kwargs)**\n",
    "\n",
    "**These arguments show up over and over again throughout the Beautiful Soup API. The most important arguments are name and the keyword arguments. The name argument restricts the set of tags by name.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterates through each product\n",
    "containers=page_soup.findAll(\"div\", {\"class\":\"item-container\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(containers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next 3 cells clean the results from findAll to find the title (brand name of the product).**\n",
    "\n",
    "**container.a retrieves everything in the a tag including href, img alt, class and title.**\n",
    "\n",
    "**container.div.div.a retrieves the a tag within 2 div attributes. **\n",
    "\n",
    "**container.div.div.a.img[\"title\"] extracts exactly the value of title from img.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"item-img\" href=\"https://www.newegg.com/Product/Product.aspx?Item=N82E16814126189&amp;ignorebbr=1\">\n",
       "<img alt=\"ASUS ROG Strix Radeon RX 570 O4G Gaming OC Edition GDDR5 DP HDMI DVI VR Ready AMD Graphics Card (ROG-STRIX-RX570-O4G-GAMING)\" class=\" lazy-img\" data-effect=\"fadeIn\" data-src=\"//images10.newegg.com/NeweggImage/ProductImageCompressAll300/14-126-189-V07.jpg\" src=\"//c1.neweggimages.com/WebResource/Themes/2005/Nest/blank.gif\" title=\"ASUS ROG Strix Radeon RX 570 O4G Gaming OC Edition GDDR5 DP HDMI DVI VR Ready AMD Graphics Card (ROG-STRIX-RX570-O4G-GAMING)\">\n",
       "</img></a>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contain=containers[0]\n",
    "container=containers[0]\n",
    "container.a #grabs everything in the a tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"item-brand\" href=\"https://www.newegg.com/ASUS/BrandStore/ID-1315\">\n",
       "<img alt=\"ASUS\" class=\" lazy-img\" data-effect=\"fadeIn\" data-src=\"//images10.newegg.com/Brandimage_70x28//Brand1315.gif\" src=\"//c1.neweggimages.com/WebResource/Themes/2005/Nest/blank.gif\" title=\"ASUS\">\n",
       "</img></a>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container.div.div.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ASUS'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container.div.div.a.img[\"title\"] #grabs the company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"item-title\" href=\"https://www.newegg.com/Product/Product.aspx?Item=N82E16814126189&amp;ignorebbr=1\" title=\"View Details\"><i class=\"icon-premier icon-premier-xsm\"></i>ASUS ROG Strix Radeon RX 570 O4G Gaming OC Edition GDDR5 DP HDMI DVI VR Ready AMD Graphics Card (ROG-STRIX-RX570-O4G-GAMING)</a>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_container=container.findAll(\"a\", {\"class\":\"item-title\"})\n",
    "title_container\n",
    "#finds the direct class: a tag, object is all classes that start with item-title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ASUS ROG Strix Radeon RX 570 O4G Gaming OC Edition GDDR5 DP HDMI DVI VR Ready AMD Graphics Card (ROG-STRIX-RX570-O4G-GAMING)'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_container[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the next cell, the name is 'li' and the attribute is a class 'price-ship'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipping_info=container.findAll(\"li\", {\"class\":\"price-ship\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li class=\"price-ship\">\n",
       "        Free Shipping\n",
       "    </li>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shipping_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\r\\n        Free Shipping\\r\\n    '"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shipping_info[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Free Shipping'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shipping_info[0].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following cell combines all the concepts used in this program above. After crawling the given pages for brand name, product name and shipping charges, we can create a csv file to keep everything in order.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand: ASUS\n",
      "product's name: ASUS ROG Strix Radeon RX 570 O4G Gaming OC Edition GDDR5 DP HDMI DVI VR Ready AMD Graphics Card (ROG-STRIX-RX570-O4G-GAMING)\n",
      "shipping charges: Free Shipping\n",
      "brand: EVGA\n",
      "product's name: EVGA GeForce GTX 1070 HYBRID GAMING, 08G-P4-6178-KR, 8GB GDDR5, LED, All-In-One Watercooling, DX12 OSD Support (PXOC)\n",
      "shipping charges: $4.99 Shipping\n",
      "brand: MSI\n",
      "product's name: MSI GeForce GTX 1060 DirectX 12 GTX 1060 GAMING X 6G Video Card\n",
      "shipping charges: $4.99 Shipping\n",
      "brand: Sapphire Tech\n",
      "product's name: SAPPHIRE NITRO+ Radeon RX 580 DirectX 12 100411NT+4G-2L Video Card w/ Backplate (UEFI), SAMSUNG MEMORY\n",
      "shipping charges: $4.99 Shipping\n",
      "brand: GIGABYTE\n",
      "product's name: GIGABYTE GeForce GTX 1060 DirectX 12 GV-N1060G1 GAMING-6GD REV 2.0 Video Card\n",
      "shipping charges: $4.99 Shipping\n",
      "brand: ZOTAC\n",
      "product's name: ZOTAC GeForce GTX 1050 DirectX 12 ZT-P10500A-10L Video Card\n",
      "shipping charges: $4.99 Shipping\n",
      "brand: ASUS\n",
      "product's name: ASUS ROG GeForce GTX 1080 STRIX-GTX1080-A8G-GAMING Video Card\n",
      "shipping charges: $4.99 Shipping\n",
      "brand: ZOTAC\n",
      "product's name: ZOTAC GeForce GTX 1050 Ti DirectX 12 ZT-P10510A-10L Video Card\n",
      "shipping charges: $4.99 Shipping\n",
      "brand: GIGABYTE\n",
      "product's name: GIGABYTE GeForce GTX 1060 DirectX 12 GV-N1060WF2OC-3GD Video Card\n",
      "shipping charges: $4.99 Shipping\n",
      "brand: MSI\n",
      "product's name: MSI Radeon RX 580 DirectX 12 RX 580 ARMOR MK2 8G OC Video Card\n",
      "shipping charges: $4.99 Shipping\n",
      "brand: EVGA\n",
      "product's name: EVGA GeForce GTX 1060 3GB SSC GAMING ACX 3.0, 03G-P4-6167-KR, 3GB GDDR5, LED, DX12 OSD Support (PXOC)\n",
      "shipping charges: $4.99 Shipping\n",
      "brand: AMD\n",
      "product's name: AMD Radeon Vega Frontier Edition 100-506061 16GB 2048-bit HBM2 Video Cards - Workstation\n",
      "shipping charges: $4.99 Shipping\n"
     ]
    }
   ],
   "source": [
    "f=open(\"NewEgg.csv\", \"w\")\n",
    "headers= \"brand, name, shipping\\n\"\n",
    "f.write(headers)\n",
    "for  container in containers: \n",
    "    brand = container.div.div.a.img[\"title\"]\n",
    "    name = container.findAll(\"a\", {\"class\":\"item-title\"})\n",
    "    name_cleaned = name[0].text\n",
    "    shipping=container.findAll(\"li\", {\"class\":\"price-ship\"})\n",
    "    shipping_cleaned=shipping[0].text.strip()\n",
    "    print (\"brand: \" + brand)\n",
    "    print (\"product's name: \" + name_cleaned)\n",
    "    print (\"shipping charges: \" + shipping_cleaned)\n",
    "    f.write(brand+ \",\" +name_cleaned.replace(\",\" , \"|\") + \",\"+ shipping_cleaned+\"\\n\")\n",
    "f.close()   #indenteation must be done correctly \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Selenium and BeautifulSoup to scrape data from CraigsList:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the packages needed to use Selenium's webdriver, WebDriverWait, Expected Conditions(EC) and the packages needed for BeautifulSoup. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following program uses selenium along with BeautifulSoup to narrow the query and scrape CraigsListaccording to the specifications provided by the user. **\n",
    "\n",
    "**We create a class whose initializer function sets the values of local variables, these will be provided by the user. This function initializes the URL and creates an instance of the Web driver from Selenium.**\n",
    "\n",
    "**The test method is included for the purposes of debugging the program. All it does is prints the URL.**\n",
    "\n",
    "**The load_craigslist_url method directs the browser to the URL given in the initializer method. We are using WebDriverWait to delay the extracting for 3 seconds. Moreover, we instruct the driver to wait for the presence of an expected condition (EC) presents itself. In this case, EC is the presence of a spefic element on the webpage. What element we are waiting for is given by ID which in this case is 'searchform'. Searchform corresponds to the entire webpage meaning that the crawler will not start extracting until the page is completely loaded. Selenium will wait until it can find the id searchform in the html. Here we searched by id, it is also possible to search by CSS element, class element, etc. The second argument specifies what type of Id we are looking for. When the page has loaded, we print a message indicating this. The page may not load instantaneouly and if the crawler tries to extract the lements before they are loaded, our program will not retrieve the desired results. To amke sure that the page was fully loaded before we extract from it, we include a delay statement However, if any exception occurs during this process, a message is printed saying that it took to long to load the page. W can handle this by increasing the delay time from 3 seconds (we could use trial and error to find out how long is too long).**\n",
    "\n",
    "**The extract_post_titles method creates a list of all the postings by class name ‘result-row’. Then we extract only the text component from each element of this list and store it in another list. Lastly, the method returns this modified list.**\n",
    "\n",
    "**The extract_post_urls method creates a list for the URLs. It receives the html code from the URL provided in the initializer method, uses BeautifulSoup’s constructor and passes in this code. Using BeautifulSoup, it is easy to collect only the links from the html script. To do this, we use a for loop to iterate through each element with the name ‘a’, attribute ‘class’, and value ‘result-title hdrlnk’ to specifically find links. The href attribute of each link is appended on to the list.**\n",
    "\n",
    "**The closing method’s only task is to close the browser.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup \n",
    "from urllib.request import urlopen\n",
    "class CraigsListScraper(object):\n",
    "    def __init__(self, location, postal, max_price, radius):\n",
    "        self.location=location\n",
    "        self.postal=postal\n",
    "        self.max_price=max_price\n",
    "        self.radius=radius\n",
    "        self.url=f\"https://{location}.craigslist.org/search/sss?search_distance={radius}&postal={postal}&max_price={max_price}\"\n",
    "        self.driver= webdriver.Firefox()\n",
    "        self.delay=3 #wait 3 seconds for all elements to load\n",
    "        \n",
    "    def test(self):\n",
    "        print(self.url)\n",
    "    \n",
    "    def load_craigslist_url(self):\n",
    "        self.driver.get(self.url)\n",
    "        try:\n",
    "            wait=WebDriverWait(self.driver, 3)\n",
    "            wait.until(EC.presence_of_element_located((By.ID, \"searchform\"))) \n",
    "            print(\"Page is ready\")\n",
    "        except TimeOutException:\n",
    "            print(\"Loading took too long\") # if this message is displayed, we need to up the delay time\n",
    "            \n",
    "    def extract_post_titles(self):\n",
    "        all_posts= self.driver.find_elements_by_class_name(\"result-row\")#result-row is the name of the class\n",
    "        post_title_list=[]\n",
    "        for post in all_posts:\n",
    "            print(post.text)\n",
    "            post_title_list.append(post.text)\n",
    "        return post_title_list\n",
    "    def extract_post_urls(self):\n",
    "        url_list = []\n",
    "        html_page = urlopen(self.url)\n",
    "        soup = BeautifulSoup(html_page, \"lxml\")\n",
    "        for link in soup.findAll(\"a\", {\"class\":\"result-title hdrlnk\"}):\n",
    "            print(link)\n",
    "            print(link[\"href\"])\n",
    "            url_list.append(link[\"href\"])#because we only want the link not the whole a tag\n",
    "        return url_list\n",
    "    def closing(self):\n",
    "        self.driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following cell represents the GUI for this program. It receives user input to specify the location, postal code, maximum price, and distance. We then create an instance of the CraigsListScraper class by passing these values. Lastly, we need to call the methods in the right order to open a browser, extract the titles of items, extarct their URLs and closing the browser. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ksu.craigslist.org/search/sss?search_distance=5&postal=66502&max_price=1500\n",
      "Page is ready\n",
      "$800\n",
      "image 1 of 4\n",
      "Mar 28 Weld Wheels and Tires $800\n",
      "$800\n",
      "image 1 of 4\n",
      "Mar 28 Weld Wheels and Tires $800 (wic > Manhattan)\n",
      "$800\n",
      "image 1 of 4\n",
      "Mar 28 Weld Wheels and Tires $800 (ksc)\n",
      "Mar 28 Aqua Cat II ( sailboat ) for sale Manhattan Ks $859 $850 (Manhattan Ks)\n",
      "$150\n",
      "image 1 of 3\n",
      "Mar 28 Patio 4 Seat Set $150\n",
      "$1000\n",
      "Mar 28 2010 Polaris 400 AWD ATV Looks Like NEw $1000 (Manhattan)\n",
      "$75\n",
      "Mar 28 iPad 2 (64 GB) & ATT LG 3TE $75\n",
      "$25\n",
      "image 1 of 2\n",
      "Mar 28 Victorian mirror with shelf $25 (Manhattan)\n",
      "$14\n",
      "image 1 of 3\n",
      "Mar 28 A BUNCH of Par 64 and 56 Light Fixtures - PRICE DROP!!! $14 (Manhattan, KS)\n",
      "$50\n",
      "image 1 of 5\n",
      "Mar 28 Electric Bass Case Road Runner $50 (Manhattan)\n",
      "$17\n",
      "image 1 of 23\n",
      "Mar 28 DRONES: 17$ Mo/ ($0 Due Today) Fly Now Pay Later DJI™Parrot™Drone $17 (Manhattan)\n",
      "Mar 28 ISO FEMALE NEW ZEALAND $1 (Manhattan)\n",
      "$20\n",
      "image 1 of 2\n",
      "Mar 28 Book shelf $20 (Manhattan)\n",
      "$5\n",
      "image 1 of 3\n",
      "Mar 28 Study lamp $5 (Manhattan)\n",
      "$30\n",
      "image 1 of 3\n",
      "Mar 28 Vacuum $30 (Manhattan)\n",
      "Mar 28 Deer feeder $425 (Manhattan)\n",
      "Mar 28 Wall painting $20\n",
      "$25\n",
      "image 1 of 2\n",
      "Mar 28 Metal ramps $25 (Manhattan)\n",
      "$15\n",
      "image 1 of 4\n",
      "Mar 28 Yamaha Electric Guitar Case $15 (Manhattan)\n",
      "$30\n",
      "image 1 of 5\n",
      "Mar 28 Les Paul Gator Guitar Case $30 (Manhattan)\n",
      "$500\n",
      "image 1 of 5\n",
      "Mar 28 Pair of Electro Voice Subs - FINAL PRICE DROP!!! $500 (Manhattan)\n",
      "$30\n",
      "image 1 of 5\n",
      "Mar 28 LOUIS VUITTON PURSE $30 (MANHATTAN KANSAS)\n",
      "$25\n",
      "image 1 of 5\n",
      "Mar 28 LADIES LIKE NEW LIFE STRIDE (BLUSH) COLOR DRESS SHOES SIZE 8M $25 (MANHATTAN)\n",
      "$25\n",
      "image 1 of 5\n",
      "Mar 28 LADIES LIFE STRIDE BLUSH COLOR DRESS SHOES (worn once)SIZE 8M $25 (MANHATTAN)\n",
      "$20\n",
      "image 1 of 4\n",
      "Mar 28 LADIES FORMAL DRESS/NAVY BLUE WITH SEQUINS SIZE 4 $20 (MANHATTAN)\n",
      "$20\n",
      "image 1 of 4\n",
      "Mar 28 LADIES FORMAL DRESS/NAVY BLUE WITH SEQUINS SIZE 4 $20 (MANHATTAN)\n",
      "$30\n",
      "image 1 of 2\n",
      "Mar 28 Chest of drawers $30 (Manhattan)\n",
      "$20\n",
      "Mar 28 Partition $20 (MANHATTAN)\n",
      "$100\n",
      "image 1 of 2\n",
      "Mar 28 $100 like new fitbit blaze smart watch $100 (Manhattan, Ks)\n",
      "$20\n",
      "image 1 of 2\n",
      "Mar 28 Lamps $20\n",
      "$20\n",
      "Mar 28 Table top foosball $20 (manhattan, ks)\n",
      "$75\n",
      "image 1 of 2\n",
      "Mar 28 $75 two 10\" subs in an enclosure $75\n",
      "$20\n",
      "Mar 28 Dining table $20 (Manhattan)\n",
      "$25\n",
      "Mar 28 Olympic barbell $25 (Manhattan)\n",
      "$75\n",
      "image 1 of 2\n",
      "Mar 28 18\" BRIDGESTONE TIRES (2) $75 (Manhattan, KS)\n",
      "Mar 28 Wanting to buy 20 to 200 acres with at least a 2 acre pond on the prop $1 (Manhattan)\n",
      "$80\n",
      "image 1 of 4\n",
      "Mar 28 Sofa ($80), Dinner table set for 4 ($100), Side table ($20) $80 (MANHATTAN)\n",
      "$100\n",
      "image 1 of 2\n",
      "Mar 28 Power wheels Barbie jeep $100 (Manhattan)\n",
      "$450\n",
      "image 1 of 2\n",
      "Mar 28 MacBook Pro 13in $450 (Junction City)\n",
      "$400\n",
      "image 1 of 22\n",
      "Mar 28 Liv Women's Comfort Bicycle: Sedona DX W - NEW! Will travel/deliver! $400 (sns > Manhattan, KS)\n",
      "$400\n",
      "image 1 of 22\n",
      "Mar 28 Liv Women's Comfort Bicycle: Sedona DX W - NEW! Will travel/deliver! $400 (lnk > Manhattan, KS)\n",
      "$400\n",
      "image 1 of 22\n",
      "Mar 28 Liv Women's Comfort Bicycle: Sedona DX W - NEW! Will travel/deliver! $400 (ksc > Manhattan, KS)\n",
      "$400\n",
      "image 1 of 22\n",
      "Mar 28 Liv Women's Comfort Bicycle: Sedona DX W - NEW! Will travel/deliver! $400 (sek > Manhattan, KS)\n",
      "$400\n",
      "image 1 of 22\n",
      "Mar 28 Liv Women's Comfort Bicycle: Sedona DX W - NEW! Will travel/deliver! $400 (tpk > Manhattan, KS)\n",
      "$400\n",
      "image 1 of 22\n",
      "Mar 28 Liv Women's Comfort Bicycle: Sedona DX W - NEW! Will travel/deliver! $400 (lwr > Manhattan, KS)\n",
      "$400\n",
      "image 1 of 22\n",
      "Mar 28 Liv Women's Comfort Bicycle: Sedona DX W - NEW! Will travel/deliver! $400 (wic > Manhattan, KS)\n",
      "$12\n",
      "Mar 28 Carpet Sweeper $12 (Manhattan, KS)\n",
      "$15\n",
      "image 1 of 5\n",
      "Mar 28 48 kids jigsaw puzzles (sold all together or in groups listed below) $15 (Manhattan, KS)\n",
      "$300\n",
      "image 1 of 3\n",
      "Mar 28 Angus Jersey calf $300 (Manhattan)\n",
      "$100\n",
      "image 1 of 2\n",
      "Mar 28 Original cdplayer/ radio 2006 Volvo S60 $100\n",
      "$220\n",
      "image 1 of 3\n",
      "Mar 28 Entertainment Centre with Storage $220 (Manhattan)\n",
      "$225\n",
      "image 1 of 3\n",
      "Mar 28 Queen frame/headboard/footboard/box spring $225 (Manhattan)\n",
      "$1\n",
      "image 1 of 7\n",
      "Mar 28 SHOP EQUIPMENT AT AFFORDABLE PRICES! 2POST 9K $2295 INSTALLED! $1 (MANHATTAN)\n",
      "$60\n",
      "image 1 of 2\n",
      "Mar 28 Round Kitchen Table $60 (Manhattan)\n",
      "$15\n",
      "Mar 28 Seashells and Starfish $15 (ksc > Manhattan)\n",
      "$20\n",
      "Mar 28 Proctor Silex Super Shooter & Spritz Cookie Press $20 (ksc > Manhattan)\n",
      "$20\n",
      "Mar 28 10 x Time Magazine. $20 (ksc > Manhattan)\n",
      "$70\n",
      "image 1 of 5\n",
      "Mar 28 Nintendo-DS-Lite (Coral Pink) MarioKart Included!!! $70 (Manhattan)\n",
      "$180\n",
      "image 1 of 4\n",
      "Mar 28 Sound System $180\n",
      "$1\n",
      "image 1 of 7\n",
      "Mar 28 Stage Lighting/Lasers/ Cymbals/Bells Bass Drum Trigger $1 (Manhattan)\n",
      "$1\n",
      "Mar 28 Didley Bow $1 (manhattan)\n",
      "$1\n",
      "Mar 28 Sports Shirts etc size M $1 (manhattan)\n",
      "$50\n",
      "Mar 28 Landscape rock $50 (Manhattan)\n",
      "$400\n",
      "image 1 of 8\n",
      "Mar 28 2 Black Hills Leather Co. western rig $400 (Manhattan)\n",
      "$30\n",
      "image 1 of 4\n",
      "Mar 28 Gerber Bear Grylls Ultimate Knife, Serrated Edge $30 (Manhattan)\n",
      "$40\n",
      "image 1 of 4\n",
      "Mar 28 Gerber LMF II Infantry Knife, Coyote Brown $40 (Manhattan)\n",
      "$30\n",
      "image 1 of 2\n",
      "Mar 28 2 Ridgid gen 4 hammer drill/drivers $30 (Manhattan)\n",
      "$25\n",
      "image 1 of 4\n",
      "Mar 28 New Water pump for Dodge Ram / Dakota $25 (Manhattan)\n",
      "$85\n",
      "image 1 of 3\n",
      "Mar 28 King mattress $85 (Manhattan)\n",
      "$400\n",
      "Mar 28 Hospital bed $400\n",
      "$125\n",
      "image 1 of 3\n",
      "Mar 28 Vortec Cylinder Heads $125\n",
      "$45\n",
      "Mar 28 Sofa Protectors $45 (Manhattan, KS)\n",
      "$30\n",
      "Mar 28 Lynksys by Cisco g wireless router $30 (manhattan, Ks)\n",
      "$45\n",
      "image 1 of 2\n",
      "Mar 28 Bridgestone Dueler AT Tires $45 (Manhattan, KS)\n",
      "$25\n",
      "Mar 28 Pier 1 metal/glass table $25 (Manhattan, KS)\n",
      "$35\n",
      "Mar 28 Candice Olson Ventura OGEE Silver Ice Quilted King Coverlet/duverlet $35 (Manhattan, KS)\n",
      "$500\n",
      "image 1 of 3\n",
      "Mar 28 MIDWEST INSTRUMENT Backflow Preventer Test Kit,Model 847 $500 (Manhattan)\n",
      "$320\n",
      "image 1 of 3\n",
      "Mar 27 New like, with warranty HP TOUCHSCREEN, 1 TB, 8GB, i3- $320 OBO $320 (Manhattan)\n",
      "$50\n",
      "image 1 of 2\n",
      "Mar 27 Vaporesso revenger x $50 (Manhattan)\n",
      "$40\n",
      "Mar 27 Gold sequins dress $40\n",
      "$80\n",
      "Mar 27 Electric Razor Scooter $80\n",
      "$20\n",
      "Mar 27 Razor Carbon Lux Kick Scooter $20\n",
      "$120\n",
      "image 1 of 5\n",
      "Mar 27 LG VK810 4G $120\n",
      "Mar 27 mobile home for storage or recycling $400 (Manhattan)\n",
      "$400\n",
      "image 1 of 3\n",
      "Mar 27 Les Paul Epiphone - Limited Edition Silver Burst $400 (Manhattan)\n",
      "$350\n",
      "Mar 27 MacBook air $350\n",
      "$50\n",
      "Mar 27 Pair of Halogen Torchere Lamps $50 (Manhattan, Ks)\n",
      "$30\n",
      "image 1 of 12\n",
      "Mar 27 Gia Milani Cheetah Purse $30 (Manhattan KS)\n",
      "$8\n",
      "Mar 27 Baby infant 3-in-1, booster, activity, and floor seat $8\n",
      "$8\n",
      "Mar 27 Baby fun table $8\n",
      "$10\n",
      "Mar 27 Wooden walker $10\n",
      "$25\n",
      "image 1 of 2\n",
      "Mar 27 Michael Kors handbag(Price reduced!) $25\n",
      "$8\n",
      "Mar 27 Two sets of baby Gates $8\n",
      "$35\n",
      "image 1 of 2\n",
      "Mar 27 Medela breast pump(Price reduced) $35\n",
      "$10\n",
      "image 1 of 2\n",
      "Mar 27 Zara boys shoe $10\n",
      "$10\n",
      "image 1 of 2\n",
      "Mar 27 First year birthday girlish stuff $10\n",
      "$10\n",
      "image 1 of 5\n",
      "Mar 27 Ladies handbag $10\n",
      "$20\n",
      "Mar 27 Baby monitor cam + receiver $20 (Manhattan)\n",
      "$300\n",
      "image 1 of 4\n",
      "Mar 27 Mizuno JPX EZ Golf Irons $300 (Manhattan)\n",
      "$60\n",
      "image 1 of 3\n",
      "Mar 27 Couch for sale $60 (Manhattan, KS)\n",
      "$325\n",
      "image 1 of 4\n",
      "Mar 27 Bowfishing Rig $325 (Manhattan)\n",
      "Mar 27 2 AMAZING tickets to NCAA Men's Basketball South Regional - TODAY $1 (Section 111 Row B)\n",
      "$250\n",
      "Mar 27 GE Washer and Dryer Set $250 (Manhattan)\n",
      "$700\n",
      "image 1 of 10\n",
      "Mar 27 1970 Ford 250 4x4 $700 (manhattan)\n",
      "$700\n",
      "image 1 of 10\n",
      "Mar 27 1970 Ford 250 4x4 $700 (tpk > manhattan)\n",
      "$700\n",
      "image 1 of 11\n",
      "Mar 27 1970 Ford 250 4x4 $700 (sns > manhattan)\n",
      "$24\n",
      "Mar 27 BRAND NEW Ling Hi SKATE BOARD (have two) $24\n",
      "$60\n",
      "image 1 of 4\n",
      "Mar 27 New K-State Powercat Handgun Pistol Leather Case $60 (Manhattan)\n",
      "$375\n",
      "image 1 of 6\n",
      "Mar 27 Mathews Switchback XT 29\" 70 Lbs $375 (Manhattan)\n",
      "Mar 27 Hardwood floor equipment $1 (Kansas)\n",
      "$15\n",
      "image 1 of 2\n",
      "Mar 27 Yoga/Exercise Mat & Ab Roller $15 (Manhttan, KS)\n",
      "$15\n",
      "image 1 of 2\n",
      "Mar 27 3 - 13.5ft Sets of Purple LED String Lights $15 (Manhattan, KS)\n",
      "$30\n",
      "image 1 of 3\n",
      "Mar 27 Black tap and jazz shoes size 10 $30\n",
      "Mar 27 Old Dell Optiplex 760 desktop computer $30\n",
      "Mar 27 Red sex link pullet baby chicks $2 (sns > Oak Hill)\n",
      "Mar 27 Red sex link pullet baby chicks $2 (tpk > Oak Hill)\n",
      "Mar 27 Red sex link pullet baby chicks $2 (Oak Hill)\n",
      "Mar 27 1997 chrysler town &counrtry mini-van $400\n",
      "Mar 27 NEW POWER LIFT RECLINER $700\n",
      "Mar 27 Rebar $1 (tpk > Manhattan)\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6511656834\" href=\"https://ksu.craigslist.org/wto/d/weld-wheels-and-tires/6511656834.html\">Weld Wheels and Tires</a>\n",
      "https://ksu.craigslist.org/wto/d/weld-wheels-and-tires/6511656834.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6511656204\" href=\"https://wichita.craigslist.org/wto/d/weld-wheels-and-tires/6511656204.html\">Weld Wheels and Tires</a>\n",
      "https://wichita.craigslist.org/wto/d/weld-wheels-and-tires/6511656204.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6511657680\" href=\"https://kansascity.craigslist.org/wto/d/weld-wheels-and-tires/6511657680.html\">Weld Wheels and Tires</a>\n",
      "https://kansascity.craigslist.org/wto/d/weld-wheels-and-tires/6511657680.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6533549820\" href=\"https://ksu.craigslist.org/boa/d/aqua-cat-ii-sailboat-for-sale/6533549820.html\">Aqua Cat  II  ( sailboat ) for  sale  Manhattan Ks  $859</a>\n",
      "https://ksu.craigslist.org/boa/d/aqua-cat-ii-sailboat-for-sale/6533549820.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545942163\" href=\"https://ksu.craigslist.org/fuo/d/patio-4-seat-set/6545942163.html\">Patio 4 Seat Set</a>\n",
      "https://ksu.craigslist.org/fuo/d/patio-4-seat-set/6545942163.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545888492\" href=\"https://ksu.craigslist.org/snw/d/2010-polaris-400-awd-atv/6545888492.html\">2010 Polaris 400 AWD ATV Looks Like NEw</a>\n",
      "https://ksu.craigslist.org/snw/d/2010-polaris-400-awd-atv/6545888492.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545912067\" href=\"https://ksu.craigslist.org/ele/d/ipad-2-64-gb-att-lg-3te/6545912067.html\">iPad 2 (64 GB) &amp; ATT LG 3TE</a>\n",
      "https://ksu.craigslist.org/ele/d/ipad-2-64-gb-att-lg-3te/6545912067.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545906109\" href=\"https://ksu.craigslist.org/atq/d/victorian-mirror-with-shelf/6545906109.html\">Victorian mirror with shelf</a>\n",
      "https://ksu.craigslist.org/atq/d/victorian-mirror-with-shelf/6545906109.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6512957352\" href=\"https://ksu.craigslist.org/msg/d/bunch-of-par-64-and-56-light/6512957352.html\">A BUNCH of Par 64 and 56 Light Fixtures - PRICE DROP!!!</a>\n",
      "https://ksu.craigslist.org/msg/d/bunch-of-par-64-and-56-light/6512957352.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6521421663\" href=\"https://ksu.craigslist.org/msg/d/electric-bass-case-road-runner/6521421663.html\">Electric Bass Case Road Runner</a>\n",
      "https://ksu.craigslist.org/msg/d/electric-bass-case-road-runner/6521421663.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545887316\" href=\"https://ksu.craigslist.org/eld/d/drones-17-mo-0-due-today-fly/6545887316.html\">DRONES: 17$ Mo/ ($0 Due Today) Fly Now Pay Later DJI™Parrot™Drone</a>\n",
      "https://ksu.craigslist.org/eld/d/drones-17-mo-0-due-today-fly/6545887316.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545864910\" href=\"https://ksu.craigslist.org/wan/d/iso-female-new-zealand/6545864910.html\">ISO FEMALE NEW ZEALAND</a>\n",
      "https://ksu.craigslist.org/wan/d/iso-female-new-zealand/6545864910.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545863080\" href=\"https://ksu.craigslist.org/fuo/d/book-shelf/6545863080.html\">Book shelf</a>\n",
      "https://ksu.craigslist.org/fuo/d/book-shelf/6545863080.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545861555\" href=\"https://ksu.craigslist.org/ele/d/study-lamp/6545861555.html\">Study lamp</a>\n",
      "https://ksu.craigslist.org/ele/d/study-lamp/6545861555.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545860642\" href=\"https://ksu.craigslist.org/hsh/d/vacuum/6545860642.html\">Vacuum</a>\n",
      "https://ksu.craigslist.org/hsh/d/vacuum/6545860642.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545814489\" href=\"https://ksu.craigslist.org/spo/d/deer-feeder/6545814489.html\">Deer feeder</a>\n",
      "https://ksu.craigslist.org/spo/d/deer-feeder/6545814489.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6517122200\" href=\"https://ksu.craigslist.org/art/d/wall-painting/6517122200.html\">Wall painting</a>\n",
      "https://ksu.craigslist.org/art/d/wall-painting/6517122200.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545778289\" href=\"https://ksu.craigslist.org/pts/d/metal-ramps/6545778289.html\">Metal ramps</a>\n",
      "https://ksu.craigslist.org/pts/d/metal-ramps/6545778289.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6521431187\" href=\"https://ksu.craigslist.org/msg/d/yamaha-electric-guitar-case/6521431187.html\">Yamaha Electric Guitar Case</a>\n",
      "https://ksu.craigslist.org/msg/d/yamaha-electric-guitar-case/6521431187.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6521398304\" href=\"https://ksu.craigslist.org/msg/d/les-paul-gator-guitar-case/6521398304.html\">Les Paul Gator Guitar Case</a>\n",
      "https://ksu.craigslist.org/msg/d/les-paul-gator-guitar-case/6521398304.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545774287\" href=\"https://ksu.craigslist.org/msg/d/pair-of-electro-voice-subs/6545774287.html\">Pair of Electro Voice Subs - FINAL PRICE DROP!!!</a>\n",
      "https://ksu.craigslist.org/msg/d/pair-of-electro-voice-subs/6545774287.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6516164149\" href=\"https://ksu.craigslist.org/clt/d/louis-vuitton-purse/6516164149.html\">LOUIS VUITTON PURSE</a>\n",
      "https://ksu.craigslist.org/clt/d/louis-vuitton-purse/6516164149.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545771079\" href=\"https://ksu.craigslist.org/hsh/d/ladies-like-new-life-stride/6545771079.html\">LADIES LIKE NEW LIFE STRIDE (BLUSH) COLOR DRESS SHOES SIZE 8M</a>\n",
      "https://ksu.craigslist.org/hsh/d/ladies-like-new-life-stride/6545771079.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6540799741\" href=\"https://ksu.craigslist.org/clo/d/ladies-life-stride-blush/6540799741.html\">LADIES LIFE STRIDE BLUSH COLOR DRESS SHOES (worn once)SIZE 8M</a>\n",
      "https://ksu.craigslist.org/clo/d/ladies-life-stride-blush/6540799741.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545712059\" href=\"https://ksu.craigslist.org/hsh/d/ladies-formal-dress-navy-blue/6545712059.html\">LADIES FORMAL DRESS/NAVY BLUE WITH SEQUINS SIZE 4</a>\n",
      "https://ksu.craigslist.org/hsh/d/ladies-formal-dress-navy-blue/6545712059.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6540780128\" href=\"https://ksu.craigslist.org/clo/d/ladies-formal-dress-navy-blue/6540780128.html\">LADIES FORMAL DRESS/NAVY BLUE WITH SEQUINS SIZE 4</a>\n",
      "https://ksu.craigslist.org/clo/d/ladies-formal-dress-navy-blue/6540780128.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6527292444\" href=\"https://ksu.craigslist.org/fuo/d/chest-of-drawers/6527292444.html\">Chest of drawers</a>\n",
      "https://ksu.craigslist.org/fuo/d/chest-of-drawers/6527292444.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6527292697\" href=\"https://ksu.craigslist.org/fuo/d/partition/6527292697.html\">Partition</a>\n",
      "https://ksu.craigslist.org/fuo/d/partition/6527292697.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6537150691\" href=\"https://ksu.craigslist.org/ele/d/100-like-new-fitbit-blaze/6537150691.html\">$100 like new fitbit blaze smart watch</a>\n",
      "https://ksu.craigslist.org/ele/d/100-like-new-fitbit-blaze/6537150691.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6537168455\" href=\"https://ksu.craigslist.org/hsh/d/lamps/6537168455.html\">Lamps</a>\n",
      "https://ksu.craigslist.org/hsh/d/lamps/6537168455.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6537181091\" href=\"https://ksu.craigslist.org/tag/d/table-top-foosball/6537181091.html\">Table top foosball</a>\n",
      "https://ksu.craigslist.org/tag/d/table-top-foosball/6537181091.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6537850136\" href=\"https://ksu.craigslist.org/pts/d/75-two-10-subs-in-an-enclosure/6537850136.html\">$75 two 10\" subs in an enclosure</a>\n",
      "https://ksu.craigslist.org/pts/d/75-two-10-subs-in-an-enclosure/6537850136.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6542140036\" href=\"https://ksu.craigslist.org/fuo/d/dining-table/6542140036.html\">Dining table</a>\n",
      "https://ksu.craigslist.org/fuo/d/dining-table/6542140036.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6542143211\" href=\"https://ksu.craigslist.org/spo/d/olympic-barbell/6542143211.html\">Olympic barbell</a>\n",
      "https://ksu.craigslist.org/spo/d/olympic-barbell/6542143211.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545682297\" href=\"https://ksu.craigslist.org/wto/d/18-bridgestone-tires-2/6545682297.html\">18\" BRIDGESTONE TIRES (2)</a>\n",
      "https://ksu.craigslist.org/wto/d/18-bridgestone-tires-2/6545682297.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6524742529\" href=\"https://ksu.craigslist.org/grd/d/wanting-to-buy-20-to-200/6524742529.html\">Wanting to buy 20 to 200 acres with at least a 2 acre pond on the prop</a>\n",
      "https://ksu.craigslist.org/grd/d/wanting-to-buy-20-to-200/6524742529.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545619253\" href=\"https://ksu.craigslist.org/fuo/d/sofa-80-dinner-table-set/6545619253.html\">Sofa ($80), Dinner table set for 4 ($100), Side table ($20)</a>\n",
      "https://ksu.craigslist.org/fuo/d/sofa-80-dinner-table-set/6545619253.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6542887637\" href=\"https://ksu.craigslist.org/bab/d/power-wheels-barbie-jeep/6542887637.html\">Power wheels Barbie jeep</a>\n",
      "https://ksu.craigslist.org/bab/d/power-wheels-barbie-jeep/6542887637.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545556519\" href=\"https://ksu.craigslist.org/sys/d/macbook-pro-13in/6545556519.html\">MacBook Pro 13in</a>\n",
      "https://ksu.craigslist.org/sys/d/macbook-pro-13in/6545556519.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6522250636\" href=\"https://salina.craigslist.org/bik/d/liv-womens-comfort-bicycle/6522250636.html\">Liv Women's Comfort Bicycle: Sedona DX W - NEW! Will travel/deliver!</a>\n",
      "https://salina.craigslist.org/bik/d/liv-womens-comfort-bicycle/6522250636.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6522242689\" href=\"https://lincoln.craigslist.org/bik/d/liv-womens-comfort-bicycle/6522242689.html\">Liv Women's Comfort Bicycle: Sedona DX W - NEW! Will travel/deliver!</a>\n",
      "https://lincoln.craigslist.org/bik/d/liv-womens-comfort-bicycle/6522242689.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6522220117\" href=\"https://kansascity.craigslist.org/bik/d/liv-womens-comfort-bicycle/6522220117.html\">Liv Women's Comfort Bicycle: Sedona DX W - NEW! Will travel/deliver!</a>\n",
      "https://kansascity.craigslist.org/bik/d/liv-womens-comfort-bicycle/6522220117.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6522258973\" href=\"https://seks.craigslist.org/bik/d/liv-womens-comfort-bicycle/6522258973.html\">Liv Women's Comfort Bicycle: Sedona DX W - NEW! Will travel/deliver!</a>\n",
      "https://seks.craigslist.org/bik/d/liv-womens-comfort-bicycle/6522258973.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6522272916\" href=\"https://topeka.craigslist.org/bik/d/liv-womens-comfort-bicycle/6522272916.html\">Liv Women's Comfort Bicycle: Sedona DX W - NEW! Will travel/deliver!</a>\n",
      "https://topeka.craigslist.org/bik/d/liv-womens-comfort-bicycle/6522272916.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6522235900\" href=\"https://lawrence.craigslist.org/bik/d/liv-womens-comfort-bicycle/6522235900.html\">Liv Women's Comfort Bicycle: Sedona DX W - NEW! Will travel/deliver!</a>\n",
      "https://lawrence.craigslist.org/bik/d/liv-womens-comfort-bicycle/6522235900.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6522278107\" href=\"https://wichita.craigslist.org/bik/d/liv-womens-comfort-bicycle/6522278107.html\">Liv Women's Comfort Bicycle: Sedona DX W - NEW! Will travel/deliver!</a>\n",
      "https://wichita.craigslist.org/bik/d/liv-womens-comfort-bicycle/6522278107.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6529577104\" href=\"https://ksu.craigslist.org/hsh/d/carpet-sweeper/6529577104.html\">Carpet Sweeper</a>\n",
      "https://ksu.craigslist.org/hsh/d/carpet-sweeper/6529577104.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6529599768\" href=\"https://ksu.craigslist.org/tag/d/48-kids-jigsaw-puzzles-sold/6529599768.html\">48 kids jigsaw puzzles (sold all together or in groups listed below)</a>\n",
      "https://ksu.craigslist.org/tag/d/48-kids-jigsaw-puzzles-sold/6529599768.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545474839\" href=\"https://ksu.craigslist.org/grd/d/angus-jersey-calf/6545474839.html\">Angus Jersey calf</a>\n",
      "https://ksu.craigslist.org/grd/d/angus-jersey-calf/6545474839.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545446569\" href=\"https://ksu.craigslist.org/pts/d/original-cdplayer-radio-2006/6545446569.html\">Original cdplayer/ radio 2006 Volvo S60</a>\n",
      "https://ksu.craigslist.org/pts/d/original-cdplayer-radio-2006/6545446569.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545442003\" href=\"https://ksu.craigslist.org/fuo/d/entertainment-centre-with/6545442003.html\">Entertainment Centre with Storage</a>\n",
      "https://ksu.craigslist.org/fuo/d/entertainment-centre-with/6545442003.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6540221716\" href=\"https://ksu.craigslist.org/fuo/d/queen-frame-headboard/6540221716.html\">Queen frame/headboard/footboard/box spring</a>\n",
      "https://ksu.craigslist.org/fuo/d/queen-frame-headboard/6540221716.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545385012\" href=\"https://ksu.craigslist.org/tld/d/shop-equipment-at-affordable/6545385012.html\">SHOP EQUIPMENT AT AFFORDABLE PRICES! 2POST 9K $2295 INSTALLED!</a>\n",
      "https://ksu.craigslist.org/tld/d/shop-equipment-at-affordable/6545385012.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545351464\" href=\"https://ksu.craigslist.org/fuo/d/round-kitchen-table/6545351464.html\">Round Kitchen Table</a>\n",
      "https://ksu.craigslist.org/fuo/d/round-kitchen-table/6545351464.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545344242\" href=\"https://kansascity.craigslist.org/for/d/seashells-and-starfish/6545344242.html\">Seashells and Starfish</a>\n",
      "https://kansascity.craigslist.org/for/d/seashells-and-starfish/6545344242.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545331352\" href=\"https://kansascity.craigslist.org/for/d/proctor-silex-super-shooter/6545331352.html\">Proctor Silex Super Shooter &amp; Spritz Cookie Press</a>\n",
      "https://kansascity.craigslist.org/for/d/proctor-silex-super-shooter/6545331352.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545312636\" href=\"https://kansascity.craigslist.org/bks/d/10-time-magazine/6545312636.html\">10 x Time Magazine.</a>\n",
      "https://kansascity.craigslist.org/bks/d/10-time-magazine/6545312636.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545281090\" href=\"https://ksu.craigslist.org/vgm/d/nintendo-ds-lite-coral-pink/6545281090.html\">Nintendo-DS-Lite (Coral Pink) MarioKart Included!!!</a>\n",
      "https://ksu.craigslist.org/vgm/d/nintendo-ds-lite-coral-pink/6545281090.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545276733\" href=\"https://ksu.craigslist.org/ele/d/sound-system/6545276733.html\">Sound System</a>\n",
      "https://ksu.craigslist.org/ele/d/sound-system/6545276733.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545276517\" href=\"https://ksu.craigslist.org/msg/d/stage-lighting-lasers-cymbals/6545276517.html\">Stage Lighting/Lasers/ Cymbals/Bells Bass Drum Trigger</a>\n",
      "https://ksu.craigslist.org/msg/d/stage-lighting-lasers-cymbals/6545276517.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545270787\" href=\"https://ksu.craigslist.org/msg/d/didley-bow/6545270787.html\">Didley Bow</a>\n",
      "https://ksu.craigslist.org/msg/d/didley-bow/6545270787.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545268424\" href=\"https://ksu.craigslist.org/clo/d/sports-shirts-etc-size/6545268424.html\">Sports Shirts etc size M</a>\n",
      "https://ksu.craigslist.org/clo/d/sports-shirts-etc-size/6545268424.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545265292\" href=\"https://ksu.craigslist.org/for/d/landscape-rock/6545265292.html\">Landscape rock</a>\n",
      "https://ksu.craigslist.org/for/d/landscape-rock/6545265292.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6521837030\" href=\"https://ksu.craigslist.org/spo/d/2-black-hills-leather-co/6521837030.html\">2 Black Hills Leather Co. western rig</a>\n",
      "https://ksu.craigslist.org/spo/d/2-black-hills-leather-co/6521837030.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6521785693\" href=\"https://ksu.craigslist.org/spo/d/gerber-bear-grylls-ultimate/6521785693.html\">Gerber Bear Grylls Ultimate Knife, Serrated Edge</a>\n",
      "https://ksu.craigslist.org/spo/d/gerber-bear-grylls-ultimate/6521785693.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6521796643\" href=\"https://ksu.craigslist.org/spo/d/gerber-lmf-ii-infantry-knife/6521796643.html\">Gerber LMF II Infantry Knife, Coyote Brown</a>\n",
      "https://ksu.craigslist.org/spo/d/gerber-lmf-ii-infantry-knife/6521796643.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545235073\" href=\"https://ksu.craigslist.org/tls/d/2-ridgid-gen-4-hammer-drill/6545235073.html\">2 Ridgid gen 4 hammer drill/drivers</a>\n",
      "https://ksu.craigslist.org/tls/d/2-ridgid-gen-4-hammer-drill/6545235073.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545233356\" href=\"https://ksu.craigslist.org/pts/d/new-water-pump-for-dodge-ram/6545233356.html\">New Water pump for Dodge Ram / Dakota</a>\n",
      "https://ksu.craigslist.org/pts/d/new-water-pump-for-dodge-ram/6545233356.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545209365\" href=\"https://ksu.craigslist.org/fuo/d/king-mattress/6545209365.html\">King mattress</a>\n",
      "https://ksu.craigslist.org/fuo/d/king-mattress/6545209365.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545198296\" href=\"https://ksu.craigslist.org/for/d/hospital-bed/6545198296.html\">Hospital bed</a>\n",
      "https://ksu.craigslist.org/for/d/hospital-bed/6545198296.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545177145\" href=\"https://ksu.craigslist.org/pts/d/vortec-cylinder-heads/6545177145.html\">Vortec Cylinder Heads</a>\n",
      "https://ksu.craigslist.org/pts/d/vortec-cylinder-heads/6545177145.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545168614\" href=\"https://ksu.craigslist.org/fuo/d/sofa-protectors/6545168614.html\">Sofa Protectors</a>\n",
      "https://ksu.craigslist.org/fuo/d/sofa-protectors/6545168614.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545165693\" href=\"https://ksu.craigslist.org/ele/d/lynksys-by-cisco-wireless/6545165693.html\">Lynksys by Cisco g wireless router</a>\n",
      "https://ksu.craigslist.org/ele/d/lynksys-by-cisco-wireless/6545165693.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545162772\" href=\"https://ksu.craigslist.org/wto/d/bridgestone-dueler-at-tires/6545162772.html\">Bridgestone Dueler AT Tires</a>\n",
      "https://ksu.craigslist.org/wto/d/bridgestone-dueler-at-tires/6545162772.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545162466\" href=\"https://ksu.craigslist.org/fuo/d/pier-1-metal-glass-table/6545162466.html\">Pier 1 metal/glass table</a>\n",
      "https://ksu.craigslist.org/fuo/d/pier-1-metal-glass-table/6545162466.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545162203\" href=\"https://ksu.craigslist.org/fuo/d/candice-olson-ventura-ogee/6545162203.html\">Candice Olson Ventura OGEE Silver Ice Quilted King Coverlet/duverlet</a>\n",
      "https://ksu.craigslist.org/fuo/d/candice-olson-ventura-ogee/6545162203.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545075959\" href=\"https://ksu.craigslist.org/grd/d/midwest-instrument-backflow/6545075959.html\">MIDWEST INSTRUMENT Backflow Preventer Test Kit,Model 847</a>\n",
      "https://ksu.craigslist.org/grd/d/midwest-instrument-backflow/6545075959.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6536247088\" href=\"https://ksu.craigslist.org/sys/d/new-like-with-warranty-hp/6536247088.html\">New like, with warranty HP TOUCHSCREEN, 1 TB, 8GB, i3- $320 OBO</a>\n",
      "https://ksu.craigslist.org/sys/d/new-like-with-warranty-hp/6536247088.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6545030183\" href=\"https://ksu.craigslist.org/ele/d/vaporesso-revenger/6545030183.html\">Vaporesso revenger x</a>\n",
      "https://ksu.craigslist.org/ele/d/vaporesso-revenger/6545030183.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6544993294\" href=\"https://ksu.craigslist.org/clo/d/gold-sequins-dress/6544993294.html\">Gold sequins dress</a>\n",
      "https://ksu.craigslist.org/clo/d/gold-sequins-dress/6544993294.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6517697088\" href=\"https://ksu.craigslist.org/bik/d/electric-razor-scooter/6517697088.html\">Electric Razor Scooter</a>\n",
      "https://ksu.craigslist.org/bik/d/electric-razor-scooter/6517697088.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6517720179\" href=\"https://ksu.craigslist.org/bik/d/razor-carbon-lux-kick-scooter/6517720179.html\">Razor Carbon Lux Kick Scooter</a>\n",
      "https://ksu.craigslist.org/bik/d/razor-carbon-lux-kick-scooter/6517720179.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6538531164\" href=\"https://ksu.craigslist.org/ele/d/lg-vk810-4g/6538531164.html\">LG VK810 4G</a>\n",
      "https://ksu.craigslist.org/ele/d/lg-vk810-4g/6538531164.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6510228033\" href=\"https://ksu.craigslist.org/grd/d/mobile-home-for-storage-or/6510228033.html\">mobile home  for  storage  or  recycling</a>\n",
      "https://ksu.craigslist.org/grd/d/mobile-home-for-storage-or/6510228033.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6544922305\" href=\"https://ksu.craigslist.org/msg/d/les-paul-epiphone-limited/6544922305.html\">Les Paul Epiphone - Limited Edition Silver Burst</a>\n",
      "https://ksu.craigslist.org/msg/d/les-paul-epiphone-limited/6544922305.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6544920715\" href=\"https://ksu.craigslist.org/sys/d/macbook-air/6544920715.html\">MacBook air</a>\n",
      "https://ksu.craigslist.org/sys/d/macbook-air/6544920715.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6544919564\" href=\"https://ksu.craigslist.org/hsh/d/pair-of-halogen-torchere-lamps/6544919564.html\">Pair of Halogen Torchere Lamps</a>\n",
      "https://ksu.craigslist.org/hsh/d/pair-of-halogen-torchere-lamps/6544919564.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6513223279\" href=\"https://ksu.craigslist.org/clo/d/gia-milani-cheetah-purse/6513223279.html\">Gia Milani Cheetah Purse</a>\n",
      "https://ksu.craigslist.org/clo/d/gia-milani-cheetah-purse/6513223279.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6527446516\" href=\"https://ksu.craigslist.org/bab/d/baby-infant-3-in-1-booster/6527446516.html\">Baby infant 3-in-1, booster, activity, and floor seat</a>\n",
      "https://ksu.craigslist.org/bab/d/baby-infant-3-in-1-booster/6527446516.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6527443221\" href=\"https://ksu.craigslist.org/bab/d/baby-fun-table/6527443221.html\">Baby fun table</a>\n",
      "https://ksu.craigslist.org/bab/d/baby-fun-table/6527443221.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6527255012\" href=\"https://ksu.craigslist.org/bab/d/wooden-walker/6527255012.html\">Wooden walker</a>\n",
      "https://ksu.craigslist.org/bab/d/wooden-walker/6527255012.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6527442506\" href=\"https://ksu.craigslist.org/clo/d/michael-kors-handbagprice/6527442506.html\">Michael Kors handbag(Price reduced!)</a>\n",
      "https://ksu.craigslist.org/clo/d/michael-kors-handbagprice/6527442506.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6527447103\" href=\"https://ksu.craigslist.org/bab/d/two-sets-of-baby-gates/6527447103.html\">Two sets of baby Gates</a>\n",
      "https://ksu.craigslist.org/bab/d/two-sets-of-baby-gates/6527447103.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6527252402\" href=\"https://ksu.craigslist.org/bab/d/medela-breast-pumpprice/6527252402.html\">Medela breast pump(Price reduced)</a>\n",
      "https://ksu.craigslist.org/bab/d/medela-breast-pumpprice/6527252402.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6527432018\" href=\"https://ksu.craigslist.org/bab/d/zara-boys-shoe/6527432018.html\">Zara boys shoe</a>\n",
      "https://ksu.craigslist.org/bab/d/zara-boys-shoe/6527432018.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6527258983\" href=\"https://ksu.craigslist.org/bab/d/first-year-birthday-girlish/6527258983.html\">First year birthday girlish stuff</a>\n",
      "https://ksu.craigslist.org/bab/d/first-year-birthday-girlish/6527258983.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6527441433\" href=\"https://ksu.craigslist.org/clo/d/ladies-handbag/6527441433.html\">Ladies handbag</a>\n",
      "https://ksu.craigslist.org/clo/d/ladies-handbag/6527441433.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6541700729\" href=\"https://ksu.craigslist.org/bab/d/baby-monitor-cam-receiver/6541700729.html\">Baby monitor cam + receiver</a>\n",
      "https://ksu.craigslist.org/bab/d/baby-monitor-cam-receiver/6541700729.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6544846015\" href=\"https://ksu.craigslist.org/spo/d/mizuno-jpx-ez-golf-irons/6544846015.html\">Mizuno JPX EZ Golf Irons</a>\n",
      "https://ksu.craigslist.org/spo/d/mizuno-jpx-ez-golf-irons/6544846015.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6544843503\" href=\"https://ksu.craigslist.org/fuo/d/couch-for-sale/6544843503.html\">Couch for sale</a>\n",
      "https://ksu.craigslist.org/fuo/d/couch-for-sale/6544843503.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6544826160\" href=\"https://ksu.craigslist.org/spo/d/bowfishing-rig/6544826160.html\">Bowfishing Rig</a>\n",
      "https://ksu.craigslist.org/spo/d/bowfishing-rig/6544826160.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6535569630\" href=\"https://ksu.craigslist.org/tix/d/2-amazing-tickets-to-ncaa/6535569630.html\">2 AMAZING tickets to NCAA Men's Basketball South Regional - TODAY</a>\n",
      "https://ksu.craigslist.org/tix/d/2-amazing-tickets-to-ncaa/6535569630.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6544811922\" href=\"https://ksu.craigslist.org/app/d/ge-washer-and-dryer-set/6544811922.html\">GE Washer and Dryer Set</a>\n",
      "https://ksu.craigslist.org/app/d/ge-washer-and-dryer-set/6544811922.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6538702419\" href=\"https://ksu.craigslist.org/cto/d/1970-ford-250-4x4/6538702419.html\">1970 Ford 250 4x4</a>\n",
      "https://ksu.craigslist.org/cto/d/1970-ford-250-4x4/6538702419.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6538706664\" href=\"https://topeka.craigslist.org/cto/d/1970-ford-250-4x4/6538706664.html\">1970 Ford 250 4x4</a>\n",
      "https://topeka.craigslist.org/cto/d/1970-ford-250-4x4/6538706664.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6538712402\" href=\"https://salina.craigslist.org/cto/d/1970-ford-250-4x4/6538712402.html\">1970 Ford 250 4x4</a>\n",
      "https://salina.craigslist.org/cto/d/1970-ford-250-4x4/6538712402.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6544802372\" href=\"https://ksu.craigslist.org/tag/d/brand-new-ling-hi-skate-board/6544802372.html\">BRAND NEW  Ling Hi SKATE BOARD (have two)</a>\n",
      "https://ksu.craigslist.org/tag/d/brand-new-ling-hi-skate-board/6544802372.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6544740159\" href=\"https://ksu.craigslist.org/spo/d/new-state-powercat-handgun/6544740159.html\">New K-State Powercat Handgun Pistol Leather Case</a>\n",
      "https://ksu.craigslist.org/spo/d/new-state-powercat-handgun/6544740159.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6544739244\" href=\"https://ksu.craigslist.org/spo/d/mathews-switchback-xtlbs/6544739244.html\">Mathews Switchback XT 29\" 70 Lbs</a>\n",
      "https://ksu.craigslist.org/spo/d/mathews-switchback-xtlbs/6544739244.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6544665266\" href=\"https://ksu.craigslist.org/wan/d/hardwood-floor-equipment/6544665266.html\">Hardwood floor equipment</a>\n",
      "https://ksu.craigslist.org/wan/d/hardwood-floor-equipment/6544665266.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6544640446\" href=\"https://ksu.craigslist.org/spo/d/yoga-exercise-mat-ab-roller/6544640446.html\">Yoga/Exercise Mat &amp; Ab Roller</a>\n",
      "https://ksu.craigslist.org/spo/d/yoga-exercise-mat-ab-roller/6544640446.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6535583959\" href=\"https://ksu.craigslist.org/hsh/d/3-135ft-sets-of-purple-led/6535583959.html\">3 - 13.5ft Sets of Purple LED String Lights</a>\n",
      "https://ksu.craigslist.org/hsh/d/3-135ft-sets-of-purple-led/6535583959.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6544564105\" href=\"https://ksu.craigslist.org/clo/d/black-tap-and-jazz-shoes-size/6544564105.html\">Black tap and jazz shoes size 10</a>\n",
      "https://ksu.craigslist.org/clo/d/black-tap-and-jazz-shoes-size/6544564105.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6544496402\" href=\"https://ksu.craigslist.org/sys/d/old-dell-optiplex-760-desktop/6544496402.html\">Old Dell Optiplex 760 desktop computer</a>\n",
      "https://ksu.craigslist.org/sys/d/old-dell-optiplex-760-desktop/6544496402.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6517704944\" href=\"https://salina.craigslist.org/grd/d/red-sex-link-pullet-baby/6517704944.html\">Red sex link pullet baby chicks</a>\n",
      "https://salina.craigslist.org/grd/d/red-sex-link-pullet-baby/6517704944.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6517709097\" href=\"https://topeka.craigslist.org/grd/d/red-sex-link-pullet-baby/6517709097.html\">Red sex link pullet baby chicks</a>\n",
      "https://topeka.craigslist.org/grd/d/red-sex-link-pullet-baby/6517709097.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6517700481\" href=\"https://ksu.craigslist.org/grd/d/red-sex-link-pullet-baby/6517700481.html\">Red sex link pullet baby chicks</a>\n",
      "https://ksu.craigslist.org/grd/d/red-sex-link-pullet-baby/6517700481.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6520828097\" href=\"https://ksu.craigslist.org/cto/d/1997-chrysler-town-counrtry/6520828097.html\">1997 chrysler town &amp;counrtry mini-van</a>\n",
      "https://ksu.craigslist.org/cto/d/1997-chrysler-town-counrtry/6520828097.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6520825824\" href=\"https://ksu.craigslist.org/fuo/d/new-power-lift-recliner/6520825824.html\">NEW POWER LIFT RECLINER</a>\n",
      "https://ksu.craigslist.org/fuo/d/new-power-lift-recliner/6520825824.html\n",
      "<a class=\"result-title hdrlnk\" data-id=\"6514593689\" href=\"https://topeka.craigslist.org/grd/d/rebar/6514593689.html\">Rebar</a>\n",
      "https://topeka.craigslist.org/grd/d/rebar/6514593689.html\n"
     ]
    }
   ],
   "source": [
    "location = \"ksu\"\n",
    "postal=\"66502\"\n",
    "max_price=\"1500\"\n",
    "radius=\"5\"\n",
    "scraper= CraigsListScraper(location, postal, max_price, radius)\n",
    "scraper.test()\n",
    "scraper.load_craigslist_url() #directs firefox to the url we just created\n",
    "scraper.extract_post_titles()\n",
    "scraper.extract_post_urls()\n",
    "scraper.closing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
