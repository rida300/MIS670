{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawling procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This tutorial provides explanations about web crawling techniques. \n",
    "**The first program is amde to extract and crawl all outgoing links from a webpage. This program:**\n",
    "**first creates a directory for the project**\n",
    "**Creates a file that stores a list of all the href links on this homepage whose url is provided by the user**\n",
    "**Next, it crawls each of these links, and once a webpage has been crawled, its url is now moved into the 'crawled' file while all the url(s) taht have yet to be crawled exist in the 'queue' file**\n",
    "**To ensure that we only crawl one given directory, our program ensures that the domain name of teh pages it crawls matches the domain of the base url. This prevents it from essentially crawling the entire internet.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next part of the tutorial is based on Selenium. We will use Selenium in order to submit assignments automatically to the school's website without having to open it and sign in each time. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from urllib.request import urlopen\n",
    "from urllib import urlopen   #is this the same as from urllib import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib2 import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from urllib2 import urlopen #doesnt work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import parse #from urllib import parse  #FIX THIS\n",
    "#from urllib.parse import urlparse #fix This"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from urllib2 import parse #doesnot work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "from queue import Queue\n",
    "from html.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import threading#spiders\n",
    "from queue import Queue#jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a method to create the project directory if it is a new project\n",
    "def create_project_dir(directory):\n",
    "    if not os.path.exists(directory):#only create if it doesn't already exist\n",
    "        print('Creating project '+ directory)\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_project_dir('theFirstProject') #only prints the name of the project when this cell is run for the first time because after that, it is already created and the if statement returns a false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating queue and crawled files(if they don't already exist)\n",
    "def create_data_files(project_name, base_url):\n",
    "    queue=os.path.join(project_name+ '/queue.txt')\n",
    "    crawled= os.path.join(project_name+'/crawled.txt')\n",
    "    if not os.path.isfile(queue):\n",
    "        write_file(queue, base_url) # so that when the program starts, it has one url in the waiting list\n",
    "    if not os.path.isfile(crawled):\n",
    "        write_file(crawled, '') #empty file so that the program knows this url has not been crawled yet\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating the new file:\n",
    "def write_file(path, data):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(data)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create_data_files('theFirstProject', 'http://us.asos.com/women/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to add data onto an existing file\n",
    "def append_to_file(path, data):\n",
    "    with open(path, 'a') as file:\n",
    "        file.write(data+'\\n') #each link on a new line  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to delete the conetnts of a file\n",
    "#creates a new file of the same name: i.e: opens that file and erases its contents\n",
    "def delete_file_contents(path):\n",
    "    open(path, 'w').close() \n",
    "#write mode selected  #pass # do nothing\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#However, this process is slower than if variables were used. The advantage of this process\n",
    "#is that if the system accidentally shuts down, we still have teh data we crawled saved kin files\n",
    "#if variables were being used, the entire process would have to be done all over again\n",
    "\n",
    "#tehrefore, we will use both: variables and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read a file and convert each line to set items\n",
    "def file_to_set(file_name):\n",
    "    results=set()\n",
    "    with open(file_name, 'rt') as f:\n",
    "        for line in f:\n",
    "            results.add(line.replace('\\n', ''))#deletes the newline part of the url\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#iterate through a set, each item in the set will be a new line in the file\n",
    "def set_to_file(links, file_name):\n",
    "    with open(file_name, \"w\") as f:\n",
    "        for link in sorted(links):#alphabetican order\n",
    "            f.write(link + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the Links on this base webpage (whose url is provided above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinkFinder(HTMLParser):\n",
    "    def __init__(self, base_url, page_url):\n",
    "        super().__init__()\n",
    "        self.base_url = base_url\n",
    "        self.page_url = page_url\n",
    "        self.links = set()\n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'a':\n",
    "            for(attribute, value) in attrs: #stored in a tuple\n",
    "                if attribute == 'href':#collecting the url of this link\n",
    "                    #converting a relative url to one with a full domain name\n",
    "                    url = parse.urljoin(self.base_url, value)#if it is a full url, it is saved as it is. Otherwise, the relative url is combined with the base url and then saved \n",
    "                    self.links.add(url)\n",
    "                    \n",
    "                    \n",
    "    def page_links(self):\n",
    "        return self.links\n",
    "                    \n",
    "    def error(self, message):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the spider class:\n",
    "**We will have a bunch of links in our waiting list, waiting to be crawled. The spider will hold on to one of these links, connect to that page and grab all of the html of this webpage and feed it to the Linkfinder object which return all of the links found in the html. Once spider has all of the links from this webpage, it makes sure that this link is not already in the waiting list and that it has not already been crawled and then it adds the link to the waiting list. Moving the webpage it has just extracted the links from into the crawled_links file is also the responsibility of the spider so we can make sure a page is not crawled twice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Spider:\n",
    "   #making aclass variable that is shared among all the spiders \n",
    "    project_name = ''\n",
    "    base_url = ''\n",
    "    domain_name=''\n",
    "    #also need to create variables for the queue and crawled files\n",
    "    queue_file=''#the actual text file\n",
    "    crawled_file=''#any spider cans et the value of these\n",
    "    queue=set()#stored on he RAM as a buffer \n",
    "    crawled=set()\n",
    "    def __init__(self, project_name, base_url, domain_name):\n",
    "        Spider.project_name= project_name\n",
    "        Spider.base_url = base_url\n",
    "        Spider.domain_name= domain_name\n",
    "        Spider.queue_file= Spider.project_name+'/queue.txt'\n",
    "        Spider.crawled_file= Spider.project_name+'/crawled.txt'\n",
    "        self.boot()\n",
    "        self.crawl_page('First spider', Spider.base_url)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def boot():#the first spider created must create the prohect directory and the 2 data files(queue and crawled)\n",
    "        create_project_dir(Spider.project_name)\n",
    "        create_data_files(Spider.project_name, Spider.base_url)#first spider therefore the url to the homepage is passed in\n",
    "        Spider.queue = file_to_set(Spider.queue_file)\n",
    "        Spider.crawled = file_to_set(Spider.crawled_file)\n",
    "        \n",
    "   \n",
    "\n",
    "    @staticmethod\n",
    "    def crawl_page(thread_name, page_url):#adding the base url to the crawled file\n",
    "        if page_url not in Spider.crawled:\n",
    "            print(thread_name+'crawling'+page_url)\n",
    "            print('Queue: '+ str(len(Spider.queue)) + ' | Crawled: '+str(len(Spider.crawled)))\n",
    "            Spider.add_links_to_queue(Spider.gather_links(page_url))\n",
    "            Spider.queue.remove(page_url)#removing this page that has just been crawled so it no longer exists in the queue(the waiting list)\n",
    "            Spider.crawled.add(page_url)\n",
    "            Spider.update_files()\n",
    "    \n",
    "    \n",
    "#The following function connects to the site, receives the html code which is initially in binary form. It \n",
    "#converts this into actual html format, passes it onto the linkFinder which parses through it and extracs all the links from this page \n",
    "    @staticmethod\n",
    "    def gather_links(page_url):\n",
    "        html_string=''\n",
    "        try:\n",
    "            response= urlopen(page_url)\n",
    "            if 'text/html' in response.getheader('Content-Type'):\n",
    "                html_bytes=response.read()#convert the 1's and 0's received from the browser into actual html format\n",
    "                html_string=html_bytes.decode(\"utf-8\")\n",
    "            finder=LinkFinder(Spider.base_url, page_url)\n",
    "            finder.feed(html_string)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            return set()\n",
    "        return finder.page_links()\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def add_links_to_queue(links):\n",
    "        for url in links:\n",
    "            if (url in Spider.queue) or (url in Spider.crawled):\n",
    "                continue\n",
    "            if Spider.domain_name != get_domain_name(url):\n",
    "                continue\n",
    "            Spider.queue.add(url)\n",
    "            \n",
    " #so that the crawler does not crawl the entire internet, the base url mist be present in all the pages being crawled\n",
    "              \n",
    "                        \n",
    "            \n",
    "            \n",
    "    @staticmethod\n",
    "    def update_files():\n",
    "        set_to_file(Spider.queue, Spider.queue_file)\n",
    "        set_to_file(Spider.crawled, Spider.crawled_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from urllib.parse import urlparse\n",
    "except ImportError:\n",
    "     from urlparse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get domain name(example.com)\n",
    "def get_domain_name(url):\n",
    "    try:\n",
    "        results= get_sub_domain_name(url).split('.')\n",
    "        return results[-2]+'.'+results[-1]#second to the last and the .com\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get subdomain name (name.example.com)\n",
    "def get_sub_domain_name(url):\n",
    "    try:\n",
    "        return urlparse(url).netloc\n",
    "    except:\n",
    "        return''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asos.com\n"
     ]
    }
   ],
   "source": [
    "print(get_domain_name('http://us.asos.com/women/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROJECT_NAME='latestProject'#constants convention allcaps\n",
    "HOMEPAGE='https://www.narscosmetics.com/'\n",
    "DOMAIN_NAME=get_domain_name(HOMEPAGE)\n",
    "QUEUE_FILE= PROJECT_NAME+'/queue.txt'\n",
    "CRAWLED_FILE = PROJECT_NAME+'/crawled.txt'\n",
    "NUMBER_OF_THREADS= 4 #SPECIFIC TO THE OPERATING SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating project latestProject\n",
      "First spidercrawlinghttps://www.narscosmetics.com/\n",
      "Queue: 1 | Crawled: 0\n",
      "addinfourl instance has no attribute 'getheader'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Spider instance at 0x000000000722CBC8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queue= Queue()\n",
    "Spider(PROJECT_NAME, HOMEPAGE, DOMAIN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawl():\n",
    "#check if there are items in the queue, if so, crawl them\n",
    "    queued_links=file_to_set(QUEUE_FILE)\n",
    "    if len(queued_links)>0:\n",
    "        print(str(len(queued_links))+ 'links in the queue')\n",
    "        create_jobs()#each queue link is a new job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_jobs():\n",
    "    for link in file_to_set(QUEUE_FILE):\n",
    "        queue.put(link)\n",
    "    queue.join()\n",
    "    crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create worker threads which will be killed once the main is exited\n",
    "def create_workers():\n",
    "    for _ in range(NUMBER_OF_THREADS):\n",
    "        t = threading.Thread(target=work)\n",
    "        t.daemon=True\n",
    "        t.start()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#do the enxt job in the queue\n",
    "def work():\n",
    "    while True:\n",
    "        url= queue.get()\n",
    "        Spider.crawl_page(threading.current_thread().name, url)\n",
    "        queue.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_workers()\n",
    "crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: creating a program to submit assignments automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('CIS450', 'HW1.txt')\n"
     ]
    }
   ],
   "source": [
    "# os for file management\n",
    "import os\n",
    "file_tup=[]\n",
    "# Build tuple of (class, file) to turn in\n",
    "submission_dir = 'completed_assignments'\n",
    "dir_list = list(os.listdir(submission_dir))\n",
    "for directory in dir_list:\n",
    "    file_list = list(os.listdir(os.path.join(submission_dir, directory)))\n",
    "    if len(file_list) != 0:\n",
    "        file_tup = (directory, file_list[0])\n",
    "    \n",
    "print(file_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "# Using Chrome to access web\n",
    "driver = webdriver.Firefox()\n",
    "driver.get('https://canvas.ksu.edu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
